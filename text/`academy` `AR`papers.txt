59-1 / H. Hua
Invited Paper
Recent Advances in Head-Mounted Light Field Displays
Hong Hua
3D Visualization and Imaging Systems Laboratory, College of Optical Sciences, The University of Arizona, 1630 E University Blvd., Tucson, AZ 85721, USA Email: hhua@optics.arizona.edu
Abstract
One of the many challenges in developing head-mounted displays (HMD) for is to create natural viewing experiences invulnerable to visual fatigue. Head-mounted light field displays render a true 3D scene by sampling either the projections of the 3D scene at different depths or the directions of the light rays apparently emitted by the 3D scene and viewed from different eye positions. They are potentially capable of rendering correct or nearly correct focus cues and addressing the very well-known vergence-accommodation mismatch problem in conventional virtual and augmented reality displays.
Keywords
Head mounted light field display; virtual reality, augmented reality
1. Introduction
Head-mounted displays (HMDs), also commonly known as near-to-eye displays (NED) or head-worn displays (HWD), have gained significant interests in recent years and stimulated tremendous efforts and resources to push the technology forward for a broad range of consumer applications. For instance, a lightweight optical see-through head-mounted display (OST- HMD), which enables optical superposition of two-dimensional (2D) or three-dimensional (3D) digital information onto a user’s direct view of the physical world and maintains see-through vision to the real world, is viewed as a transformative technology in enabling new ways of accessing and perceiving digital information essential to our daily life. In recent years significant advancements have been made toward the development of compact HMDs for both augmented reality (AR) and virtual reality (VR) applications, and several HMD products are commercially deployed or close to commercial launch.
Despite the high promises and the tremendous progresses made recently toward the development of both VR and AR displays, minimizing visual discomfort involved in wearing HMDs for an extended period remains to be an unresolved challenge. One of the key contributing factors to visual discomfort is the well- known vergence-accommodation conflict due to the lack of the ability to render correct focus cues. The vergence cue refers to the rotation action of the eyes to bring the visual axes inward or outward to intersect at a 3D object of interest at near or far distances. The accommodation cue refers to the focus action of the eye where ciliary muscles change the refractive power of the crystalline lens and minimize the amount of blur for the fixated depth of the scene. Associated with eye accommodation change is the retinal image blur effect in which objects away from the eye’s accommodation depth appear blurry in the retinal image due to the limited depth-of-field (DOF) of the eyes. In natural viewing, the degree of image blur varies with the distance of the object from the focused distance. The accommodation and
retinal image blur effects together are known as the focus cues. The accommodation and retinal blur together are often referred to as the focus cues.
In recent years, several technical methods have been explored to address the VAC problem in conventional HMDs, attempting to approximate the visual effects created by the focus cues in natural vision. Based on the different optical mechanisms for controlling focus cues, Marran and Schor described five different methods that can potentially minimize the visual discomfort associated with conflicting accommodative cues in HMDs, namely pinhole optics, monocular lens addition, monocular lens addition combined with aniso-accommodation, chromatic bifocal, and bifocal lens systems [1]. Although the above classification method is still valid to some extent today, to account for the recent advancements in both optical technologies and computational displays, we categorize the existing approaches into three general types: Maxwellian view displays, vari-focal plane displays, and light field displays [2].
Among the there methods, the Maxwellian view display attempts to extend the DOF of the virtual display to the extent such that eye accommodation is no longer required to view the virtual scene [3-5]. This allows the eye to accommodate and converge freely to view real-world scenes, without compromising the image sharpness of the virtual display, which partially resolves the vergence-accommodation conflict in OST- HMDs. However, it is unable to produce natural retinal blur cues to virtual scenes. The vari-focal approach attempts to adapt the focus of the display in real-time to the vergence depth corresponding to the region of interest [6]. It can be a simple and effective modification to conventional HMDs and is able to address the VAC problems to a great extent, but it is unable to produce natural retinal blur cues. The head-mounted light field displays render a true 3D scene by sampling either the projections of the scene at different depths or the directions of the light rays apparently emitted by the scene and viewed from different eye positions. In this article, we focus on reviewing recent advancements of head-mounted light field displays for VR and AR applications.
2. Light Field Display Methods
A light field 3D (LF-3D) display renders the perception of a 3D object (e.g. the cube) by reproducing the directional samples of the light rays apparently emitted by each point on the object such that multiple ray samples are viewed through each of the eye pupils, as illustrated in Fig. 1. The light intensities corresponding to the angular samples of the light rays are anisotropically modulated to reconstruct a 3D scene that approximates the visual effects of viewing a natural 3D scene. To enable the eye to accommodate at the depth of the 3D object rather than the source from which the rays are actually originated, a true LF-3D display requires multiple different elemental views are seen through each of the eye pupils and they integrally sum up to form the perception of the object. When the
872 • SID 2017 DIGEST ISSN 0097-996X/17/4702-0872-$1.00 © 2017 SID

      Invited Paper
eye is accommodated at a given depth (e.g. the blue corner of the cube), the directional ray samples apparently emitted by a point of the same depth naturally form a sharply focused image on the retina, while the rays for points at other depths (e.g. the red and green corners) form retinal blur effects varying with their corresponding depth. Therefore, this type of LF-3D displays has the potential to render correct or nearly correct focus cues for 3D scenes and resolve the VAC problem. Several different methods have been explored to implement this type of head-mounted LF-3D displays, including integral-imaging (InI) based displays [7-9] and computational multi-layer light field displays [10,11]. An InI-based HMD, utilizing the same principle as integral photography technique invented by Lipmamn in 1908 [12], typically consists of a microdisplay panel and a 2D array which can be a micro-lens array (MLA) [7,8] or aperture array [9]. While the 2D array angularly samples the directional light rays of a 3D scene, the display renders a set of 2D elemental images, each of which represents a different perspective of the 3D scene through each MLA or aperture. The conical ray bundles emitted by the corresponding pixels in the elemental images intersect and integrally create the perception of a 3D scene that appears to emit light and occupy the 3D space. An InI-based display using 2D arrays allows the reconstruction of a 3D shape with full-parallax information in both horizontal and vertical directions. A computational multi-layer display is a relatively new, emerging class of LF-3D display method that samples the directional rays through multi-layers of pixel arrays. It typically employs a stack of light-attenuating layers illuminated by a backlight [10,11]. The light field of a 3D scene is computationally decomposed into a number of masks representing the transmittance of each layer of the light attenuators. The intensity value of each light ray entering the eye from the backlight is the product of the pixel values of the attenuation layers at which the ray intersects. The computational multi-layer displays operate in a multiplicative fashion and approximate the light fields of the scene by angularly sampling the directions of the light rays apparently emitted by a 3D scene in a fashion similar to the InI-based display. The pinlight display, consisting of a spatial light modulator (SLM) and an array of point light sources, namely the pinlights, may also be considered as a computational multi-layer display where the back layer of light attenuators next to the backlight is replaced by a pinhole array or an array of point sources [5].
Fig. 1. Schematic illustration of a light field display which reproduces the directional light rays apparently emitted by the object of a given position.
Alternative to the angular sampling strategy illustrated in Fig. 1, a LF-3D display may also render a 3D scene volume by rendering a stack of discrete 2D images to sample the projections of the 3D scene at different depth along the view direction, which is known as multi-focal plane (MFP) displays [13-16]. The focal plane stack divides the extended scene volume into multiple zones, each of which only renders 3D objects nearby. Fig. 2 shows a schematic diagram of a dual-focal plane example. The MFP method is an extension to the bifocal
59-1 / H. Hua
lens method described earlier in [1] by providing a larger number of focal plane sampling and 3D continuity. An MFP- based display may be implemented either by spatially multiplexing a stack of 2D displays [14] or by fast-switching the focal distance of a single 2D display sequentially by a high- speed vari-focal element (VFE) in synchronization with the frame rendering of multi-focal images [15,16]. For instance, Akeley et al. demonstrated a proof-of-concept bench prototype of a three-focal plane display covering a fixed depth range from 0.311m to 0.536m [14] by dividing a flat panel display into three focal planes through 3 beamsplitters placed at different distance to the viewer. Using a liquid lens as the VFE and an OLED microdisplay as the image source, Liu and Hua demonstrated the first prototype of a dual-focal plane optical see-through AR display, which maintains a non-obstructive see-through view to the real world [15].
Fig. 2. Schematic illustration of a dual-focal plane display which renders two discrete focal planes along the z- direction to sample the 2D projections of the 3D scene at different depths.
In the conventional MFP display methods, a large number of focal planes and small dioptric spacing are desirable for achieving accurate focus cues. It was suggested that the dioptric spacing between adjacent focal planes should be 1/7 diopters to achieve 1 arc minute spatial resolution [13]. At this spacing, 28 focal planes are needed to cover the depth range from infinity to 4 diopters. However, it is practically very challenging to achieve a large number of focal planes with the affordance of current technologies. Reducing the number of focal planes without noticeably compromising the quality of displayed images and the accuracy of rendered focus cues can have significant advantages. To achieve good image quality with a significantly reduced number of focal planes, the MFP approach may be further improved by incorporating a depth-fused 3D (DFD) perception, where two overlapped images displayed on two transparent screens at two different depths may be perceived as a single-depth image, as illustrated in Fig. 3. The perceived depth of the fused image may be manipulated by modulating the luminance ratio between the two images through depth- weighting fusing function to optimize the contrast magnitude and gradient of the fused image which create cues for stimulating and stabilizing the eye accommodation responses. A multi-focal plane display incorporating the DFD effect is thus referred to as depth-fused MFP displays, or DFD-MFP displays. This method significantly reduces the number of necessary focal planes to an affordable level to render correct or nearly-correct focus cues for 3D objects spanning in a large depth range to the viewer [17-19]. Liu and Hua [17], Hu and Hua [18,20], and Hu [19] presented an optimization framework for DFD-MFP system design, in which the focal planes spacing and the fusion
   SID 2017 DIGEST • 873

59-1 / H. Hua
functions are optimized to maximize the contrast magnitude and gradient of the retinal image fused by the two images overlapping along the visual axis to avoid conflicting focusing cues and create smooth contrast gradient that helps to stimulate and stabilize the eye accommodation response. Hu and Hua designed and built six-focal plane optical see-through HMD prototypes to demonstrate the effectiveness of the DFD-MFP method and the fusion functions [20]. Natural focus cues were demonstrated and high-contrast continuous targets were correctly fused across 6 spatially separated focal planes. Both the virtual display and optical see-through path of the prototype also achieved superb performance.
(a) (b)
Fig. 3. Schematic model of a DFD-MFP display: the luminance ratio between the front and back images are modulated to change the perceived depth of the fused image to be near to the front image plane (a) and near the back image (b).
3. Conclusion
This article provided an overview of different methods toward the development of head-mounted light field displays for augmented and virtual reality applications. Despite the tremendous progress in the past decades, all of the existing technical approaches are subject to different tradeoffs and limitations. The micro-InI based light field display approach is able to render correct focus cues for a large depth volume, but its optical performances, including spatial resolution, depth of field, longitudinal resolution, and angular resolution, are relatively low compared to the MFP approach. The computational multi-layer approach is still in its preliminary development stage and requires significant innovations to overcome some fundamental limitations such as diffraction artifacts. The multi-focal plane (MFP) display method is capable of rendering correct focus cues for a 3D scene across a large depth volume at high spatial resolution comparable to conventional non-light field HMD methods, but it has to overcome several critical obstacles to become a compact, wearable display solution. Significant innovations have to be made to engineer head-mounted LF-3D displays with compact and portable form factor and high optical performance.
4. Acknowledgement
Part of the work reviewed is funded by National Science Foundation Grant Awards 0915035 and 1422653. Dr. Hong Hua has a disclosed financial interest in Magic Leap Inc. The terms of this arrangement have been properly disclosed to The University of Arizona and reviewed by the Institutional Review Committee in accordance with its conflict of interest policies.
5. References
[1] L. Marran and C. Schor, “Multiaccommodative stimuli in VR Systems: Problems & Solutions,” Human Factors, 39(3), 382-388, 1997.
Invited Paper
[2] H. Hua, “Enabling focus cues in head-mounted displays,” Proceedings of the IEEE, 2017.
[3] M. von Waldkirch, P. Lukowicz, and G. Tr ̈oster, “LCD- based coherent wearable projection display for quasi accommodation-free imaging,” Opt. Commun. 217:133–40, 2003.
[4] A. Yuuki, K. Itoga, T. Satake, “A new Maxwellian view display for trouble-free accommodation,” Journal of the SID, 20(10): 581-588, 2012.
[5] A. Maimone, D. Lanman, K. Rathinavel, K. Keller, D. Luebke, H. Fuchs, “Pinlight displays: wide field of view augmented reality eyeglasses using defocused point light sources,” ACM Trans. On Graphics, 33(4) 89:1-11, 2014.
[6] S. Liu, H. Hua, and D. Cheng, “A novel prototype for an optical see-through head-mounted display with addressable focus cues.,” IEEE Trans. Vis. Comput. Graph. 16, 381–393, 2010.
[7] H. Hua and B. Javidi, "A 3D integral imaging optical see- through head-mounted display," Opt. Exp. 22, 13484-91 2014.
[8] D. Lanman and D. Luebke, “Near-eye light field displays,”
ACM Transaction on Graphics), 2013.
[9] W. Song, Y. Wang, D. Cheng, and Y. Liu, “Light field head- mounted display with correct focus cue using micro structure array,” Chinese Optics Letters, 12(6) 060010, 2014.
[10]A. Maimone, G. Wetzstein, M. Hirsch, D. Lanman, R. Raskar, and H. Fuchs, “Focus 3d: compressive accommodation display,” ACM Trans. Graphics, 32(5): 153:1-153:3, 2013.
[11] F. Huang, K. Chen, G. Wetzstein. “The Light Field Stereoscope: Immersive Computer Graphics via Factored Near- Eye Light Field Displays with Focus Cues”, ACM SIGGRAPH (Transactions on Graphics), 33(5), 2015.
[12] G. Lippmann, “Epreuves reversibles donnant la sensation
du relief,” J. phys. (Paris) 7, 821–825, 1908.
[13] Rolland, J. P., Kureger, M., and Goon, A., “Multifocal planes head-mounted displays,” Applied Optics, 39(19), pp. 3209-14, 2000.
[14] K. Akeley, S. J. Watt, A. R. Girshick, and M. S. Banks, “A Stereo Display Prototype with Multiple Focal Distances,” ACM Trans. Graph. 23, 804–813, 2004.
[15] Sheng Liu and Hong Hua, “Time-multiplexed dual-focal plane head-mounted display with a fast liquid lens,” Optics Letter, 34(11):1642-44, 2009.
[16] G. D. Love, D. M. Hoffman, P. J. W. Hands, J. Gao, A. K. Kirby, and M. S. Banks, “High-speed switchable lens enables the development of a volumetric stereoscopic display,” Opt. Express 17(18), 15716–15725, 2009.
[17] S. Liu and H. Hua, “A systematic method for designing depth-fused multi-focal plane three-dimensional displays,” Opt. Express, 18(11), 11562–11573, 2010.
[18] X. Hu and H. Hua, “Design and Assessment of a Depth- Fused Multi-Focal-Plane Display Prototype,” Disp. Technol. J. , 2014.
[19] Hu, X., Development of the Depth-Fused Multi-Focal Plane Display Technology, Ph.D. Dissertation, College of Optical Sciences, University of Arizona, 2014.
[20] Xinda Hu and Hong Hua, “High-resolution optical see- through multi-focal-plane head-mounted display using freeform optics,” Optics Express, 22(11): 13896-13903, June 2014.
  874 • SID 2017 DIGEST

 See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/318144095
46-1: Dual-layer High Dynamic Range Head Mounted Display
Article in SID Symposium Digest of Technical Papers · May 2017 DOI: 10.1002/sdtp.11725
CITATION READS
1 32
  2 authors:
Miaomiao Xu
Facebook
17 PUBLICATIONS SEE PROFILE
102 CITATIONS
Hong Hua
The University of Arizona
175 PUBLICATIONS 3,069 CITATIONS SEE PROFILE
    Some of the authors of this publication are also working on these related projects:
OCOST-HMD View project
Connected components, Eye tracking View project
   All content following this page was uploaded by Miaomiao Xu on 02 March 2020. The user has requested enhancement of the downloaded file.
 
Dual-layer High Dynamic Range Head Mounted Display
Miaomiao Xu, Hong Hua*
3D Visualization and Imaging Systems Laboratory, College of Optical Sciences, The University of Arizona, 1630 E University Blvd., Tucson, AZ 85721, USA Email: mmxu@optics.arizona.edu, *hhua@optics.arizona.edu
Abstract
In this paper, a high dynamic range (HDR) head mounted display system using a dual-layer modulation configuration was proposed. By using two reflective spatial light modulators simultaneously, we demonstrated the ability to achieve an HDR head mounted display with a contrast ratio as high as 60000:1.
Keywords
Head mounted display; high dynamic range (HDR); dual layer modulation
1. Introduction
The dynamic range or contrast ratio of a display is defined as the number of discrete intensity levels that the display can render. Most of the state-of-art full-color displays are only capable of rendering 8-bits of discrete intensity levels per color channel and thus have a very limited ability to reproduce the broad dynamic range of real-scenes, while the human visual system can comfortably view a luminance range from as low as 0.1 cd/m2 to
2
as high as 10,000 cd/m . Compared with the high dynamic range
of the human eye, the capability of generating luminance contrast range by today’s displays is far below by several orders of magnitude. The most common way of displaying a high dynamic range (HDR) image on traditional display devices is to adopt a tone-mapping technique, which converts the HDR image into an image with much smaller dynamic range while maintaining the image integrity. Many different kinds of tone-mapping operators, such as global operator, local operator, frequency operator and gradient operator have been developed [2-5]. Although these tone mapping operators can make HDR images accessible through conventional displays of nominal dynamic range, the image contrast ratio is significantly reduced to the limit of the display dynamic range and the discernable grayscales are limited to 256 levels per color channel.
Instead of trying different methods to compress the image contrast range, researchers have been seeking for hardware solutions that are able to maintain the image contrast ratio and the discernable grayscales by developing HDR displays. Intuitively, there are two ways to design a HDR display. One way is to widen the dynamic range of the display pixels by increasing the maximally displayable luminance values and increasing the bit depth per color channel. However, it may require high-amplitude, high-resolution drive circuit or high peak illumination devices, which is currently not readily available for manufacture [6]. Another more realistic way is to combine two or more layers of modulators to simultaneously control the pixel output values. Seetzen et al. [1] demonstrated a dual-layer modulation method for desktop HDR display by combining a back-lit liquid-crystal display (LCD) panel with a DLP projector or an LED array where the DLP projector or the LED array serves as the backlight to the LCD. They demonstrated an HDR display system that had a dynamic range beyond 50,000:1. More recently, Hirsch et al. [7] demonstrated an HDR projector by using compressive light field method.
Head-mounted displays are subject to the same limitation of dynamic range as other display systems, but few efforts have been made to address the dynamic range challenge. In this paper, we present a compact HDR head mounted display (HDR-HMD) system by using a double-layer modulation configuration. By using two layers of reflective liquid crystal on silicon (LCoS) microdisplays as spatial light modulators to simultaneously modulate a light source, we demonstrate an HDR-HMD with per- pixel modulation capability, potentially offering a contrast ratio over 60000:1.
2. Methods
Our proposed HDR-HMD design adopted the dual-layer modulation method in [1]. In principle, stacking two transmissive liquid crystal display (LCD) panels along with a backlight source might be considered as the most straightforward and compact adaptation to an HMD framework, which would share the similar hardware configuration to the light field stereoscope approach by Wetzstein et al [8]. Unlike the light field display approach where the LCD layers are spaced apart for light field rendering, here the LCD layers for HDR rendering need to be placed close to each other and the light source is successively modulated by the two LCDs for dynamic range enhancement. The resulting dynamic range can be as high as (CL1*CL2):1, where CL1 and CL2 are the dynamic range of the two LCDs, respectively.
The direct stacking method, however, suffers from several critical limitations. First of all, transmissive LCDs tend to have low dynamic range and low transmittance. The stacked dual-layer modulation would lead to very low light efficiency and limited dynamic range enhancement. Secondly, transmissive LCDs tend to have a low fill factor and the microdisplays utilized in HMDs typically have pixels as small as a few microns. As a result, the light transmitting through a two-layer LCD stack will inevitably suffer from severe diffraction effects and yield poor image resolution following an eyepiece magnification. Thirdly, due to the physical construction of LCD panels, the modulation layers of the LCD panels will be inevitably separated by a gap as large as a few millimeters, depending on the thickness of the panel. Even a gap as small as 1 millimeter in the display stack will result a large separation in the visual space after the HMD eyepiece magnification (e.g. approximately 2.5 meters separation with a 50x eyepiece lateral magnification), which makes accurate dynamic range modulation practically impossible.
To overcome the aforementioned problems with a dual-layer LCD stack, we propose to use two reflective type microdisplays such as LCoS along with a relay optical structure. The reflective nature of LCoS displays not only offers higher contrast ratio and higher light efficiency, but also offers much higher fill-factor (typically over 90%) than LCDs. High pixel fill factor helps effectively minimize the diffraction artifacts when stacking multiple layers of modulation. Due to the reflective nature of LCoS displays, however, it is practically impossible to place two LCoS panels close to each other. A large separation between the modulation layers will inevitably limit the range of dynamic

range modulation. Based on the consideration of its effectiveness, a relay system was designed so that we can relay the image of one LCoS to the exact position of the other. As a result, the images of the two displays could optically close to or even overlap with each other, which enables the ability to achieve per-pixel dynamic range modulation.
Figure 1 shows the schematic layout of the proposed system and optical path for each LCoS. The two LCoS panels are identical, with a size of 10.16mm in diagonal, 1280*960-pixel resolution, and 6.35-micron pixel size. The LCoS1 has an in-built LED illumination and with a wire grid film (WGF) at the front. The emitted light by the LED is firstly modulated by the LCoS1, then passes through the relay system as well as a polarized beam splitter (PBS), and is modulated by the LCoS2 for the second time. Based on the principle of light coupling modulation, the overall dynamic range of our system is the multiplication of the two LCoS dynamic range. As the LCoS panel acts as a combination of a mirror and a quarter wave retarder, the light modulated by the LCoS2 is reflected by the PBS toward an eyepiece. Finally, the modulated image is magnified by the eyepiece for viewing.
Figure 1. Schematic layout of a dual-layer HDR-HMD system.
2.2 system design requirement
In order to make the two LCoS optically aligned close to each other, a relay system with unit magnification need to be designed. Due to the nature of liquid crystal material, LCoS has its own limited viewing angle. That means both the image contrast and quality would be significantly degraded once viewing beyond that angle. As for the relay system, it would have little impact if the system is faster than the viewing angle correspondence. The LCoS we proposed in our system has the ±10° viewing angle, which corresponding to have 0.176 object space numerical aperture (NA). Besides, a system with double telecentric scheme was also desired. For one reason, the illumination light comes from the backlighting of LCoS1, and would also come to be the light engine for LCoS2. This configuration needs to have uniform illumination distribution, even when the light cone reached the side of LCoS2. For another reason, the image magnification should not be changed once the position of LCoS is deviated a little from its nominal image plane. A double telecentric relay system could offer both uniform illumination and magnification, which makes it as a requirement of our system design. Other requirement such as external dimension of the system, the total track of the system should also be well controlled for mounting convenience.
3. Result
3.1 Relay system design and bench prototype
In order to make the two LCoS panels optically aligned close to each other, a relay system with a unit magnification is required.
Additionally, due to the reflective nature of LCoS panels, the light incident angles on the LCoS panels need to be constrained within a range to avoid degradation of image contrast. The incidence angle limit for the LCoS microdisplays we adopted in our system is about ±10°, which corresponds to a numerical aperture (NA) limit of 0.176 in the display space. Finally, it is highly desirable that the relay system is double telecentric to achieve high light modulation efficiency, uniform image brightness across the field of view of the HMD system, high tolerance to mis-alignment errors between the two LCoS panels.
Based on all the requirements mentioned above, a double telecentric relay system with all lenses matched with stock lenses was designed. In order to further control the aberration without increase system total length, we adopted a tunable aperture stop which makes the system NA changeable from 0.125 to 0.176. A 12.7mm cubic polarized beam splitter was put at the front of LCoS2 for light path folding. 470nm, 550nm and 610nm were the primary emitting wavelengths for RGB LED sources and were selected as the primary wavelengths for optimization. Figure 2 shows the layout of the relay system with system NA set as 0.125. After the optimization process, the polychromatic modulation transfer function (MTF) was over 0.25 at the cut-off frequency of 78.7 cycles/mm. The residual distortion is less than 1.52% at the edge of the panel. The diameter of the relay tube is only 25mm and total system length is about 123mm.
Figure 2. Layout of the relay system design.
Figure 3 shows a photograph of our bench prototype. The mechanical part of the relay tube was 3D printed. Two LCoS microdisplay panels were fixed onto miniature optical platform with fine adjustments of their orientations and positions. They were placed face to face with a relay tube and PBS in between. A stock eyepiece was utilized as the viewing optics. A digital camera was placed at the exit pupil of the system where a viewer’s eye would be located.
Figure 3. High dynamic range system setup.
3.2 Image alignment and distortion correction
One important part that would influence the HDR image quality was the alignment of the two LCoS panels. The image quality would be degraded severely if the positions of two microdisplays deviated from their nominal positions, even with pixel-level
   
 Figure 4. HDR image generation. The images in the black frame showed the original images captured by camera with different exposure time. The exposures was denoted at the upper left of each image, unit in ms. The bottom right image showed the merged HDR image with tone-mapping technique.
displacement. This makes it difficult to align the two LCoS panels by only adjusting their relative positions manually. Furthermore, owing to the different light path for the two LCoS panels through the system, the image for each LCoS is subject to different optical distortion coefficients, which requires proper correction to achieve per-pixel dynamic range control. Even when the center field of the two microdisplays had been perfectly aligned, image misalignment at the edge of the full field can still be observed if distortion is not corrected.
In order to align the two nominal image position pixel-by-pixel, we developed a two-step calibration method by adapting our camera-based display calibration method [10]. The LCoS2 image was set to be the reference image plane. In order to overlay the two image locations for a certain viewing position within the exit pupil of the system, a homographic technique was used to project LCoS1 image onto the nominal image plane of LCoS2. The projection matrix was calibrated by using camera based HMD calibration mentioned in [10]. The distortion coefficient for the LCoS2 can be calibrated simultaneously during this step. To get the distortion coefficients for the LCoS1 image with respective to the new projection position, the second calibration was necessary based on its current image position. Through the two-step calibration, the average alignment error between the two LCoS panels was maintained less than one pixel across the entire visual field.
3.3 HDR image generation
In order to reproduce and display an HDR image by HDR-HMD system, the raw image data source with a large enough dynamic range is required. One of the most common ways to generate HDR images is to take a series of pictures with different exposure time or stop aperture, synthesize all the collected radiance information and then merge them into a single HDR image. This
HDR image is able to contain fine structures and details in both highlights and shadows. Figure 4 shows the process of generating a HDR image source for our HDR-HMD system. The targeted HDR image was made by capturing a well-light scene with multiple exposures, then synthesized and stored in a HDR image format. In this example, ten pictures were taken with different exposure times while keeping all the other parameters unchanged. It can be clearly seen that the features on the brighter outdoor scene backgrounds were captured with shorter exposures while the details of the front indoor objects were captured with long exposure times. With such HDR image generation technique, features and details both in dark and bright areas can be stored within one picture, due to the extended dynamic range strorage. To show the final HDR image synthetic performance in a conventional low dynamic range image, a tone-mapped image was shown at bottom right of Figure 4.
3.4 HDR image rendering and experimental results
To redistribute the image modulation functionality onto the two SLMs in our HDR-HMD system, each pixel value needs to be recalculated based on their luminance information. In order to display the image with expected radiance distribution to be seen by the viewers, an HDR image rendering algorithm was developed to generate the images shown on both displays. Another issue for the HDR-HMD system was the correction of some field-dependent illumination un-uniformity, which might be caused by vegnetting, camera sensation, or backlighting un- uniformity. To overcome this issue, we added a backlighting calibration and adjustment procedure to correct the illumination uniformity during the LCoS1 image rendering process.
Figure 5 shows a comparison of images captured through the system by adopting different rendering and display technique. In

 Figure 5. Comparison between resulting images with different display techniques through the system. (a) HDR HMD with HDR image; (b) LDR HMD with HDR tone- mapping image; (c) and (d) LDR HMD with LDR images, (c) and (d) showed captured images when displaying LDR images with 1ms and 2ms exposure time respectively.
the targeted scene, several objects with different shapes and features were exhibited indoors. The intensity of the indoor light was much weaker, compared with the outdoor scene overshined with sunlight. Figure 5(a) shows a captured image of the HDR targeted scene displayed by the proposed HDR display system where the input image is the raw HDR image generated in Sec. 3.3. As a comparison, Figure 5(b) shows a captured image of the tone-mapped targeted scene, rather than a true high dynamic range image, displayed by the proposed HDR-HMD system with LCoS2 modulation disabled. Similarly, Figures 5(c) and (d) show the captured images of two low dynamic range (LDR) targeted scenes, which were originally captured with 1ms and 2ms camera exposure times, respectively, displayed by the proposed HDR- HMD system with LCoS2 modulation disabled. It is worth noting that the images shown in Figure 5 were captured and synthesized using the same process to make them directly comparable. For each display condition, we captured the display scenes using four different exposures and these images were synthesized and tone- mapped using the same process as described in Sec. 3.3. With an HDR display, we could clearly see both the indoor and outdoor scene with gradual command level change from Fig. 5(a), due to the enlarged contrast range of the HDR display, though the overall image contrast is not as high as the original HDR source due to the tone-mapping compressing. With a tone-mapped image source on a LDR display, it is hard to see the features and grey level distinctions on the foreground objects in Figure 5(b), especially the areas where the features have narrower grey level variation. This is more obviously shown on the left part of the image, where most details and features on the small blocks and the can were lost, due to the dynamic range compression in tone- mapping process and contrast ratio depressing when displaying with a LDR display. As expected from a LDR image source and display, Figures 5(c) and (d) failed to show both outdoor and indoor scenes simultaneously because most radiance information has been lost with a single exposure and storage of the narrow dynamic range of the targeted scene. The tone-mapped image shown by an LDR display (Fig. 5(b)) is noticeably better than the
results of LDR input to the same display, but noticeably worse than the resolute shown by a true HDR display (Fig. 5(a)).
4. Impacts
In this paper, a dual-layer high dynamic range head mounted display system was proposed. A new rendering algorithm for HDR display was implemented. Instead of using tone mapping technique to compress the original image contrast range, truly high dynamic range images can be displayed in head mounted display system. Both intensity and dynamic range of our display was significantly superior to the traditional displays.
An HDR-HMD system has great potentials in many aspects. This display prototype offers higher contrast ratio, and pixel-by-pixel modulation makes the best use of the modulation capacity of two individual displays. The alignment procedure was developed to guarantee the displayed image quality. Moreover, the radiance map and image rendering algorithm was proposed for displaying a image with desired luminance.
5. Acknowledgement
This research was partially funded by National Science Foundation grant award 14-22653. Dr. Hong Hua has a disclosed financial interest in Magic Leap Inc. The terms of this arrangement have been properly disclosed to The University of Arizona and reviewed by the Institutional Review Committee in accordance with its conflict of interest policies.
6. References
[1] Seetzen H, Heidrich W, Stuerzlinger W, et al. High dynamic range display systems[J]. ACM Transactions on Graphics (TOG), 2004, 23(3): 760-768.
[2] Durand F, Dorsey J. Fast bilateral filtering for the display of high-dynamic-range images[C]//ACM transactions on graphics (TOG). ACM, 2002, 21(3): 257-266.
[3] Larson G W, Rushmeier H, Piatko C. A visibility matching tone reproduction operator for high dynamic range scenes[J]. IEEE Transactions on Visualization and Computer Graphics, 1997, 3(4): 291-306.
[4] Salih Y, Malik A S, Saad N. Tone mapping of HDR images: A review[C]//Intelligent and Advanced Systems (ICIAS), 2012 4th International Conference on. IEEE, 2012, 1: 368- 373.
[5] Reinhard E, Stark M, Shirley P, et al. Photographic tone reproduction for digital images[J]. ACM transactions on graphics (TOG), 2002, 21(3): 267-276.
[6] High-dynamic-range (HDR) vision[M]. Springer Berlin Heidelberg, 2007.
[7] Hirsch M, Wetzstein G, Raskar R. A compressive light field projection system[J]. ACM Transactions on Graphics (TOG), 2014, 33(4): 58.
[8] G. Wetzstein, D. Lanman, M. Hirsch, R. Raskar. Tensor Displays: Compressive Light Field Synthesis using Multilayer Displays with Directional Backlighting. Proc. of SIGGRAPH 2012 (ACM Transactions on Graphics 31, 4), 2012.
[9] http://www.3dsystems.com/quickparts
[10] Lee S, Hua H. A robust camera-based method for optical distortion calibration of head-mounted displays[J]. Journal of Display Technology, 2015, 11(10): 845-853.
View publication stats
Distinguished Student Paper 72-1 / H. Huang An Integral-imaging-based Head Mounted Light Field Display
Using a Tunable Lens and Aperture Array
Hekun Huang, Hong Hua*
3D Visualization and Imaging Systems Laboratory, College of Optical Sciences, The University of Arizona, 1630 E University Blvd., Tucson, AZ 85721, USA Email: *hhua@optics.arizona.edu
Abstract
A head-mounted light field display based on integral imaging is considered as one of the promising methods that can render correct or nearly correct focus cues and address the well- known vergence-accommodation conflict problem in head mounted displays. Despite its great potential, it still suffers some of the same limitations of conventional integral imaging based displays such as low spatial resolution and crosstalk. In this paper, we present a prototype design using tunable lens and aperture array to render 3D scenes over a large depth range while maintaining high image quality and minimizing crosstalk. Experimental results verify and show that the proposed design could significantly improve the viewing experience.
Keywords
Head mounted light field display; integral imaging; tunable lens; aperture array
1. Introduction
Since it was first proposed by Lippmann in 1908 [1], Integral imaging (InI) has been widely researched for various applications [2-4]. When applied to 3D displays, it is capable of rendering a true 3D scene by sampling the directions of the light rays apparently emitted by the 3D scene and viewed from different eye positions and shows advantages over stereoscopic or holographic approaches. InI-based 3D displays can work with incoherent light sources and does not produce speckle unlike holography. More importantly it is capable of providing full parallax and can potentially render correct or nearly correct focus cues and mitigate the well-known vergence- accommodation conflict [5], which is anticipated to reduce distorted perception of depth [6] as well as visual discomfort and fatigue [7].
By taking advantage of the emerging technology of head mounted display (HMD) and microscopic integral imaging (micro-InI) method, it is possible to create a lightweight and compact optical see-through head-mounted display (OST-HMD) solutions [8,9] that are less vulnerable to the aforementioned accommodation-convergence discrepancy problem. However, like other InI-based display technologies, the simple micro-InI based HMD architecture consisting of a micro-lenslet array (MLA) and a micro-display suffers from several major limitations due to the very nature of the integral imaging such as the low spatial resolution through a large depth volume and crosstalk between the elemental images. Several methods have been explored to address these issues, such as using a varifocal liquid lens array [10], or a bifocal liquid crystal lens [11] to enhance the depth, and using movable pinhole array [12] or a periodic black mask with a layer of high refractive index packing medium [13] to enhance the viewing window and equivalently reduce the crosstalk. Though all of these
approaches help to improve the performance of the traditional integral imaging display system, it might be difficult, if not impractical, to directly transfer such techniques to an integral imaging head mounted display (InI-HMD) due to their complexity and various differences between conventional displays and HMDs.
In this paper, we present a cost-effective, practical method and implementation of an InI-HMD using a tunable lens and an aperture array, which is capable of rendering a reconstructed 3D scene for a relatively large depth range without degrading the image quality as well as a reasonable viewing window without noticeable crosstalk.
2. Methods
Our approach is based on the work by Hua and Javidi [8], where a reconstructed 3D scene formed by a micro-InI unit consisting of a high-resolution micro-display and a microlens array (MLA) is directly coupled into and magnified by an eyepiece for viewing. Each elemental image distributed on the micro-display functions as spatially-incoherent object of which the ray bundles of the pixels will intersect at certain depths through its corresponding MLA and integrally create the perception of a 3D scene. Such an InI-HMD allows the reconstruction of a 3D surface shape with parallax information in both horizontal and vertical directions, and has the potential to provide a large 3D volume (FOV by depth) with high spatial resolution compared to conventional InI-based display systems. However, there still exist several limitations of the prior arts. First, although with the benefits of HMD viewing optics magnification, a relatively narrow depth range is adequate for the intermediate 3D scene reconstructed by the micro-InI unit to produce a perceived 3D volume spanning a large depth range, it is still difficult to maintain a relatively high spatial resolution of the scene over such a depth range due to the defocus and diffraction effect of the MLA. The spatial resolution of the reconstructed 3D scene quickly degrades three to five times in the edge of a depth range around 3 diopters to that in the center, which will significantly reduce the displayed image contrast. Secondly, the viewing window, or viewing zone of an InI-HMD is well restrained to minimize the crosstalk between the neighboring elemental images on the micro-display due to the very nature of the InI display. The size of a cross-talk-free viewing zone is much smaller than that of conventional HMDs of similar eyepiece and thus will largely compromise the viewing experience. In order to solve these two aforementioned issues, we modify the overall InI-HMD system by further adopting a tunable lens and an aperture array which will be discussed in details in the following sections.
2.1 Expanding the depth using a tunable lens
Just as in any other HMDs, achieving high spatial resolution always becomes one of the top priorities in designing an InI-
ISSN 0097-996X/17/4702-1049-$1.00 © 2017 SID SID 2017 DIGEST • 1049

                        72-1 / H. Huang
Distinguished Student Paper
            based light field HMD system to provide the best possible viewing quality to render fine details. Therefore, the resolution- priority InI strategy rather than the depth-priority strategy which is preferred in conventional InI-based display systems is selected for an InI-based light field HMD [8, 14]. Instead of placing the micro-display at the front focal point of the MLA to create collimated light and consequently constant but poor spatial resolution in the depth of field (DOF) in the depth-priority strategy, for resolution-priority strategy, the microdisplay is placed either in front of or behind the front focal point of the MLA such that a maximum spatial resolution is typically obtained on a central depth plane (CDP) which is optically conjugate to the micro-display. The CDP will act as a reference plane to the reconstruction of a 3D scene to fully exploit the resolving power of the system. However, such a setup will induce an inevitable degradation of the spatial resolution when the reconstruction depth is displaced away from the depth of the CDP due to the defocus and diffraction effects of the MLA. Such degradation will pass to the virtual CDP and other planes after being magnified through the eyepiece. There inevitably exists a trade-off between spatial resolution and DOF in resolution-priority strategy [15] and usually to achieve a reasonable DOF it ends up with low spatial resolution.
Figure 1 shows the schematic layout of the proposed system using a tunable lens to expand the DOF of an InI-HMD system while maintaining a relatively high spatial resolution. The system mainly consists of three key parts: a micro-InI unit reproducing the full-parallax light fields of a 3D scene, a vari- focal relay group to create an intermediate 3D scene with a tunable position of its central depth plane (CDP), and an eyepiece optics reimaging the tunable 3D light fields into a viewer’s eye. The vari-focal relay group consists of a front and a rear relay lens and a tunable lens sandwiched in-between. By adjusting the optical power of the tunable lens, the intermediate reconstructed 3D scene will shift axially with respect to the eyepiece. By placing the tunable surface inside the tunable lens at the back focal length of the front relay lens and making it an optical conjugate to the exit pupil of the eyepiece where the entrance pupil of the eye is placed to view the display, the compound optical power of the relay group is maintained constantly independent of the optical power of the tunable lens, which helps the virtually reconstructed 3D scene achieve constant field of view regardless of the focal depths of the CDP. Therefore, a much larger volume of a 3D scene could be visually perceived without seams or artifacts with the rapidly changing optical power of the tunable lens as well as the corresponding contents in the micro-display.
2.2 Reducing crosstalk using aperture array Traditionally in an InI-based display system the viewing
window, also referred to as the viewing angle, is calculated by
only accounting for the chief rays from the pixels of the integral images through the corresponding lenslets in MLA [15]. Though usually it is not a big issue in conventional InI-based display system, the viewing window of an InI-HMD defined in such a way will inevitably introduce crosstalk from neighboring elemental images, and thus significantly affect the viewing experience. As illustrated in Figure 2, the upper marginal ray (Ray I) of a pixel on the margin of elemental image 2 (EI2) through the lenslet corresponding to elemental image 1 (EI1) will still transfer through the system stop and consequently the exit pupil (viewing window), even though the chief ray (Ray II) as well as the lower marginal ray (Ray III) of it is blocked as designed in conventional integral imaging display system. It will lead to the leakage, or crosstalk of subpart of the EI2 upon the location where only the EI1 is supposed to be imaged, and such phenomenon will simultaneously apply to the elemental images all through the reconstructed scene. To prevent this, usually the size of the viewing window has to be reduced by a fraction similar to that in a floating InI display systems [16].
Fig.2 Illustration of the formation of the crosstalk
Fig.1 Schematic layout of the proposed system using the tunable lens
 1050 • SID 2017 DIGEST
Fig.3 Illustration of the working principle of the proposed system using the aperture array
Figure 3 shows the schematic layout of the proposed system using an aperture array (For simplicity, only the micro-InI unit is

         Distinguished Student Paper
illustrated) to reduce the crosstalk alternatively. Compared to the traditional setup, now a thin, paper-like aperture array is inserted between the micro-display and MLA. The working principle of the aperture array is also illustrated in Figure 3. The small-angle crosstalk of the neighboring elemental images will be completely blocked from entering the corresponding lenslet in MLA by each of the micro-apertures in the aperture array (area A in Figure 3, EI2 to EI1). In comparison, though it is possible for the large-angle crosstalk passing through the aperture array (area B in Figure 3, EI2 to EI3), this kind of crosstalk actually is far beyond the viewing angle and thus will be totally blocked by the system stop instead, while the transmittance of the rays from the pixels of the corresponding elemental image through it will be allowed with only slight vignetting on the edge of the elemental image (area C in Figure 3). By doing so, the crosstalk between the neighboring elemental images could be eliminated and the ghost-image-like phenomenon will be minimized at a slight cost of image brightness.
3. Setup and results
A bench prototype of the proposed InI-HMD system incorporating both of the tunable lens and the aperture array was built as shown in Figure 4(a). The magnification of the relay group is 1:1, and an electrically tunable liquid lens (Optotune EL-10-30) was inserted at the back focal point of the front relay lens and set as the system stop. An aperture array printed on a transparent paper with predefined pattern matching the size of the microlenses of the MLA and elemental images on micro- display, as shown in Figure 4(b), was attached to the front surface of the MLA and well aligned.
Fig.4 Image of (a) bench top setup, (b) aperture array
To verify the capability of extending the DOF of proposed system, a simple 3D scene composed of four groups of letter ‘E’s with three different spatial sizes and orientations is simulated. In the micro-InI space, they are spaced approximately 1mm apart counterclockwise which corresponds to a depth from 0 to 3 diopters with a step of 1 diopter following the magnification by the eyepiece, and the distance indicators (number ‘0’, ‘1’, ‘2’, ‘3’ in red) are set at the CDP and consequently the virtual CDP, of which the depth may vary according to the setup. Though the four groups of letters are rendered at different depths, they subtend the same angular
72-1 / H. Huang dimensions to demonstrate their comparable spatial resolution in
the visual space.
Figure 5(a)-(d) show the captured images of the 3D scene displayed by the proposed system, where at each time only one of the groups was displayed (except in Figure 5(a) all the groups were displayed for reference), and more importantly the virtual CDP was adjusted by the tunable lens in accordance to the depth of the corresponding group. In other words, though the four groups of letter ‘E’s were at different depths, by applying time- multiplexing scheme they could be all rendered on the virtual CDP sequentially using the tunable lens. Besides, the overall arrays of elemental images for the micro-display were also rendered separately to account for the depth differences. The camera was adjusted to focus on 1, 0, 2, 3 diopters from Figure 5(a)-(d), respectively. In this case, all the groups of letter ‘E’s with the same depth of camera (in red box) of the captured images were in focus and the image quality was significantly improved. Equivalently the DOF of the system was significantly extended.
Fig.5 Captured image of simulated 3D scene in proposed setup when camera focuses on (a) 1 diopter, (b) 0 diopter, (c) 2 diopters and (d) 3 diopters
A new pattern was created as a resolution target to verify the capability of reducing the crosstalk of the proposed system, where all the letters (‘3’, ‘D’, ‘V’, ‘I’, ‘S’) were on or near the CDP to rule out the influence of defocus and divergence of the elemental images. The virtual CDP was fixed at 1 diopter as well as the focus plane of the camera. The aperture and exposure time of the camera was also fixed for all the images captured. Figure 6(a) shows the captured image of the target when no physical stop aperture was set on the tunable lens, where severe crosstalk could be clearly observed. By inserting a stop of which the diameter remains large matching only the chief ray of the pixels through the MLA as in traditional InI display, crosstalk could be largely reduced but still visible, as shown in Figure 6(b). Further shrinking the size of the stop could help wipe out the crosstalk, as shown in Figure 6(c), but it also inevitably reduced the brightness through the scene. However, as shown in Figure 6(d), by using the aperture array between the microdisplay and MLA, the diameter of the viewing window could be maintained to the same level as in Figure 6(b)
                   SID 2017 DIGEST • 1051

72-1 / H. Huang
but the crosstalk fully eliminated at a slight cost of image
brightness.
Fig.6 Captured image of simulated 3D scene of proposed setup (a) with no physical stop, (b) with a large system stop, no aperture array, (c) with small system stop, no aperture array, (d) with large system stop and aperture array
4. Conclusion
By incorporating both the tunable lens and aperture array, our method could boost the displaying quality as well as the viewing experience of InI-HMD by enhancing the depth range and reducing crosstalk. In addition, compared to other approaches, our method could be more easily implemented into a compact InI-HMD without significantly increasing the complexity of the system.
We believe that our method has great potentials for developing an InI-HMD that has the ability of rendering authentic 3D scene to the viewer without sacrificing any important viewing parameters such as resolutions or size of exit pupil (viewing window) compared to other OST-HMDs. Such an InI-HMD could a necessary tool that allows the investigation of the effects of focus cues on depth perception and visual fatigue in a systematic manner.
5. Acknowledgement
This research was partially funded by National Science Foundation grant award 14-22653 and a Google Faculty Research Award. Dr. Hong Hua has a disclosed financial interest in Magic Leap Inc. The terms of this arrangement have been properly disclosed to The University of Arizona and reviewed by the Institutional Review Committee in accordance with its conflict of interest policies.
Distinguished Student Paper [1] G. Lippmann, “Epreuves reversibles donnant la sensation du
6. References
relief,” J. Phys. (Paris) 7, 821–825 (1908).
    [2] A. Stern and B. Javidi, “Three dimensional image sensing, visualization, and processing using integral imaging,” Proc. IEEE, vol. 94, no. 3, pp. 591–607, (2006).
[3] R. Martinez-Cuenca, Raul, G. Saavedra, M. Martinez-Corral, and B. Javidi, “Progress in 3-D multiperspective display by integral imaging,” Proc. IEEE., vol. 97, no. 6, pp. 1067–1077, (2009).
[4] X. Xiao, B. Javidi, M. Martinez-Corral, and A. Stern, “Advances in three-dimensional integral imaging: Sensing, display, and applications,” Appl. Opt., vol. 52, pp. 546–560, (2013).
[5] D.M. Hoffman, A.R. Girshick, K. Akeley, and M.S. Banks, “Vergence-Accommodation Conflicts Hinder Visual Performance and Cause Visual Fatigue,” J. Vision, 8(3), 1-30, (2008).
[6] S. J. Watt, K. Akeley, M. O. Ernst, and M. S. Banks, “Focus cues affect perceived depth,” J. Vision, vol. 5, no. 10, pp. 7–7, (2005).
[7] J. Sheedy and N. Bergstrom, “Performance and Comfort on Near-Eye Computer Displays,” Optometry and Vision Science, 79(5), 306-312, (2002).
[8] H. Hua and B. Javidi, "A 3D integral imaging optical see- through head-mounted display," Opt. Express 22, 13484-13491 (2014)
[9] W. Song, Y. Wang, D. Cheng, and Y. Liu, "Light field head- mounted display with correct focus cue using micro structure array," Chin. Opt. Lett. 12, 060010- (2014)
[10] C. Kim, M., M. Lee, J. Kim, and Y. Won, "Depth plane adaptive integral imaging using a varifocal liquid lens array," Appl. Opt. 54, 2565-2571 (2015)
[11] X. Shen, Y. Wang, H. Chen, X. Xiao, Y. Lin, and B. Javidi, "Extended depth-of-focus 3D micro integral imaging display using a bifocal liquid crystal lens," Opt. Lett. 40, 538-541 (2015)
[12] Y. Kim, J. Kim, J. Kang, J. Jung, H. Choi, and B. Lee, "Point light source integral imaging with improved resolution and viewing angle by the use of electrically movable pinhole array," Opt. Express 15, 18253-18267 (2007)
[13] C. Luo, C. Ji, F. Wang, Y. Wang, and Q. Wang, "Crosstalk- Free integral imaging display with wide viewing angle using periodic black mask," J. Display Technol. 8, 634-638 (2012)
[14] J. Jang, F. Jin, and B. Javidi, "Three-dimensional integral imaging with large depth of focus by use of real and virtual image fields," Opt. Lett. 28, 1421-1423 (2003)
[15] J. Park, S. Min, S. Jung, and B. Lee, "Analysis of viewing parameters for two display methods based on integral photography," Appl. Opt. 40, 5217-5232 (2001)
[16] J. Kim, S. Min, and B. Lee, "Viewing region maximization of an integral floating display through location adjustment of viewing window," Opt. Express 15, 13023-13034 (2007)
            1052 • SID 2017 DIGEST
  Systematic characterization and optimization of 3D light field displays
HEKUN HUANG AND HONG HUA*
3D Visualization and Imaging Systems Laboratory, College of Optical Sciences, University of Arizona, 1630 East University Boulevard, Tucson 85721, Arizona, USA
*hhua@optics.arizona.edu
Abstract One of the key issues in conventional stereoscopic displays is the well-known vergence-accommodation conflict problem due to the lack of the ability to render correct focus cues for 3D scenes. Recently several light field display methods have been explored to reconstruct a true 3D scene by sampling either the projections of the 3D scene at different depths or the directions of the light rays apparently emitted by the 3D scene and viewed from different eye positions. These methods are potentially capable of rendering correct or nearly correct focus cues and addressing the vergence-accommodation conflict problem. In this paper, we describe a generalized framework to model the image formation process of the existing light-field display methods and present a systematic method to simulate and characterize the retinal image and the accommodation response rendered by a light field display. We further employ this framework to investigate the trade-offs and guidelines for an optimal 3D light field display design. Our method is based on quantitatively evaluating the modulation transfer functions of the perceived retinal image of a light field display by accounting for the ocular factors of the human visual system.
© 2017 Optical Society of America
OCIS codes: (120.2040) Displays; (330.4060) Vision modeling; (330.7322) Visual optics, accommodation.
References and links
1. J. P. Wann, S. Rushton, and M. Mon-Williams, “Natural problems for stereoscopic depth perception in virtual environments,” Vision Res. 35(19), 2731–2736 (1995).
2. D. M. Hoffman, A. R. Girshick, K. Akeley, and M. S. Banks, “Vergence-accommodation conflicts hinder visual performance and cause visual fatigue,” J. Vis. 8(3), 1–30 (2008).
3. S. J. Watt, K. Akeley, M. O. Ernst, and M. S. Banks, “Focus cues affect perceived depth,” J. Vis. 5(10), 834–862 (2005).
4. M. Mon-Williams, J. P. Warm, and S. Rushton, “Binocular vision in a virtual world: visual deficits following the wearing of a head-mounted display,” Ophthalmic Physiol. Opt. 13(4), 387–391 (1993).
5. J. F. Heanue, M. C. Bashaw, and L. Hesselink, “Volume holographic storage and retrieval of digital data,” Science 265(5173), 749–752 (1994).
6. G. E. Favalora, J. Napoli, D. M. Hall, R. K. Dorval, M. G. Giovinco, M. J. Richmond, and W. S. Chun, “100 million-voxel volumetric display,” Proc. SPIE 4712, 300–312 (2002).
7. G. E. Favalora, “Volumetric 3D displays and application infrastructure,” Computer 38(8), 37–44 (2005).
8. K. Akeley, S. J. Watt, A. R. Girshick, and M. S. Banks, “A stereo display prototype with multiple focal
distances,” ACM Trans. Graph. 23(3), 804–813 (2004).
9. S. Liu and H. Hua, “A systematic method for designing depth-fused multi-focal plane three-dimensional
displays,” Opt. Express 18(11), 11562–11573 (2010).
10. X. Hu and H. Hua, “Design and Assessment of a Depth-Fused Multi-Focal-Plane Display Prototype,” J. Disp.
Technol. 10(4), 308–316 (2014).
11. Y. Takaki and N. Nago, “Multi-projection of lenticular displays to construct a 256-view super multi-view
display,” Opt. Express 18(9), 8824–8835 (2010).
12. Y. Kajiki, H. Yoshikawa, and T. Honda, “Hologram-like video images by 45-view stereoscopic display,” Proc.
SPIE 3012, 154–166 (1997).
13. Y. Takaki, “High-density directional display for generating natural three-dimensional images,” Proc. IEEE
94(3), 654–663 (2006).
14. J. H. Lee, J. Park, D. Nam, S. Y. Choi, D. S. Park, and C. Y. Kim, “Optimal projector configuration design for
300-Mpixel multi-projection 3D display,” Opt. Express 21(22), 26820–26835 (2013).
15. A. Jones, I. McDowall, H. Yamada, M. Bolas, and P. Debevec, “Rendering for an interactive 360° light field
display,” ACM Trans. Graph. 26(3), 40 (2007).
#293267 https://doi.org/10.1364/OE.25.018508
Journal © 2017 Received 20 Apr 2017; revised 4 Jul 2017; accepted 16 Jul 2017; published 24 Jul 2017
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18508
 
 16. H. Arimoto and B. Javidi, “Integral three-dimensional imaging with digital reconstruction,” Opt. Lett. 26(3), 157–159 (2001).
17. H. Hua and B. Javidi, “A 3D integral imaging optical see-through head-mounted display,” Opt. Express 22(11), 13484–13491 (2014).
18. W. Song, Y. Wang, D. Cheng, and Y. Liu, “Light field head-mounted display with correct focus cue using micro structure array,” Chin. Opt. Lett. 12(6), 060010 (2014).
19. W. Gordon, D. Lanman, M. Hirsch, and R. Raskar, “Tensor displays: compressive light field synthesis using multilayer displays with directional backlighting,” ACM Trans. Graph. 31, 80 (2012).
20. A. Maimone, D. Lanman, K. Rathinavel, K. Keller, D. Luebke, and H. Fuchs, “Pinlight displays: wide field of view augmented reality eyeglasses using defocused point light sources,” ACM Trans. Graph. 33(4), 89 (2014).
21. Y. Kim, J. Kim, K. Hong, H. K. Yang, J. Jung, H. Choi, S. Min, J. Seo, J. Hwang, and B. Lee, “Accommodative Response of Integral Imaging in Near Distance,” J. Disp. Technol. 8(2), 70–78 (2012).
22. S. K. Kim, D. W. Kim, Y. M. Kwon, and J. Y. Son, “Evaluation of the monocular depth cue in 3D displays,” Opt. Express 16(26), 21415–21422 (2008).
23. A. Stern, Y. Yitzhaky, and B. Javidi, “Perceivable light fields: Matching the requirements between the human visual system and autostereoscopic 3D displays,” Proc. IEEE 102(10), 1571–1587 (2014).
24. G. Lippmann, “Epreuves reversibles donnant la sensation du relief,” J. Phys. (Paris) 7, 821–825 (1908).
25. J. Schwiegerling, Field Guide to Visual and Ophthalmic Optics (SPIE, 2004).
26. https://optics.synopsys.com/
27. J. E. Greivenkamp, J. Schwiegerling, J. M. Miller, and M. D. Mellinger, “Visual acuity modeling using optical
raytracing of schematic eyes,” Am. J. Ophthalmol. 120(2), 227–240 (1995).
28. D. G. Green and F. W. Campbell, “Effect of focus on the visual response to a sinusoidally modulated spatial
stimulus,” J. Opt. Soc. Am. A 55(9), 1154–1157 (1965).
29. J. S. Jang and B. Javidi, “Improvement of viewing angle in integral imaging by use of moving lenslet arrays with
low fill factor,” Appl. Opt. 42(11), 1996–2002 (2003).
30. C. Jang, K. Hong, J. Yeom, and B. Lee, “See-through integral imaging display using a resolution and fill factor-
enhanced lens-array holographic optical element,” Opt. Express 22(23), 27958–27967 (2014).
1. Introduction
Conventional stereoscopic three-dimensional displays (S3D) stimulate the perception of 3D space and shapes from a pair of two-dimensional (2D) perspective images, one for each eye, with binocular disparities and other pictorial depth cues of a 3D scene seen from two slightly different viewing positions. Although they can create compelling depth perceptions, the S3D- type displays are subject to a well-known vergence-accommodation conflict (VAC) problem due to the inability to render correct focus cues, including accommodation and retinal blur effects, for 3D scenes [1,2]. Eye accommodation refers to the focus action of the eye where ciliary muscles contract or relax to change the refractive power of the crystalline lens to obtain and maintain clarity of retinal image for a fixated object of a given depth. The retinal image blur effect refers to the phenomena associated with eye accommodation change in which objects away from the eye’s accommodative distance appear blurry in the retinal image due to the limited depth-of-field (DOF) of the eyes. The conventional S3D displays fail to render correct retinal blur effects and stimulate natural eye accommodation response, which causes several cue conflicts and is considered as one of the key contributing factors to various visual artifacts associated with viewing S3D displays, such as distorted depth perception [3] and visual discomfort [4]. In recent years, several display methods that are potentially capable of resolving the VAC problem have been demonstrated, including holographic displays [5], volumetric displays [6,7], multi-focal plane displays [8–10] and light field displays [11–20]. Among these different methods, the light field display method is considered as one of the most promising 3D display techniques.
As illustrated in Fig. 1, a light field 3D (LF-3D) display, which in some cases is referred to as super multi-view (SMV) displays, renders the perception of a 3D object (e.g. the cube) by reproducing the directional samples of the light rays apparently emitted by each point on the object such that multiple ray samples are viewed through each of the eye pupils. The light intensities corresponding to the angular samples of the light rays are anisotropically modulated to reconstruct a 3D scene that approximates the visual effects of viewing a natural 3D scene. Each of the directional samples represents the subtle difference of the object when viewed from slightly different positions and thus is regarded as an elemental view of the
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18509
 
 object. To enable the eye to accommodate at the depth of the 3D object rather than the source from which the rays are actually originated, a true LF-3D display requires that multiple different elemental views are seen through each of the eye pupils and they integrally sum up to form the perception of the object. When the eye is accommodated at a given depth (e.g. the blue corner of the cube), the directional ray samples apparently emitted by a point of the same depth naturally form a sharply focused image on the retina, while the rays for points at other depths (e.g. the red and green corners) form retinal blur effects varying with their corresponding depth. Therefore, a LF-3D display has the potential to render correct or nearly correct focus cues for 3D scenes and resolve the VAC problem. Furthermore, it can also potentially render correct motion parallax, which is the change in retinal images caused by the change of eye pupil position, and further improve 3D perception. Conventional two-view S3D displays lack the ability to render motion parallax because the retinal images remain unchanged when the eyes move within each of their own viewing windows.
Fig. 1. Schematic illustration of a light field display which reproduces the directional light rays apparently emitted by the object of a given position.
Despite that several research groups have demonstrated the promising potentials of LF-3D displays for addressing the VAC problem, there are very few systematic investigations upon addressing many of the fundamental issues in the process of engineering a LF-3D display, including (1) methods for quantifying the accuracy of the focus cues rendered by a LF-3D display; (2) the threshold requirement for a LF-3D display to render correct or nearly correct focus cues; and (3) the optimal configurations for a LF-3D display that provide both correct accommodative cue and high image quality. Several pioneering works attempted to address some of these issues. Takaki suggested that the angular separation of the directional ray bundles is required to be around 0.2°~0.4° to allow more than two view samples for each eye and to stimulate accommodation response, where the eyes are anticipated to focus at the rendered depth instead of the 2D screens [13]. It remains unclear that how accurate the rendered focus cues would be if such a view density is satisfied. Kim et al. experimentally measured the accommodative responses in viewing a real object and a digital 3D object rendered through integral imaging (InI) and suggested that over 73% of the 71 participants were able to accommodate at the depth of the rendered object instead of a display screen [21]. Kim et al. captured the point spread function (PSF) of different 3D displays and evaluated the DOF and the monocular focus cues of these displays based on their PSF measurements [22]. Stern at al. attempted to combine major perception and human visual system requirements with analytical tools to build an analytical framework for establishing perceivable light fields and determining display device specifications [23]. Overall, however, a critical gap is the lack of a systematic method to quantify the relationships between the accuracy of focus cue rendering and the number of samples per pupil area and a systematic investigation on the trade-off relations between view numbers and retinal image quality.
In this paper, we present a systematic approach to fully address the three aforementioned fundamental issues in designing a LF-3D display. By extracting the common characteristics of the existing LF-3D methods, we present a generalized framework to model their image formation process (Section 2), based on which we present a systematic method to simulate
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18510
 
 and characterize the retinal image quality (Section 3) and the accommodative response (Section 4) perceived from a LF-3D display by accounting for both the ocular and display factors. We further employ this framework to investigate the optimal view sampling strategy for LF-3D display designs offering balance between accommodation cue accuracy and retinal image quality (Section 5). Our method is based on quantitative evaluation of the modulation transfer functions (MTF) of the perceived retinal image of light-field displays. Instead of performing an observer-based, objective measurement, we adopt a schematic eye model which takes into account most of the ocular factors such as pupil size, eye aberrations, diffraction and accommodation. Based on the MTF of the light-field images and the DOF of the human visual system, we determine the retinal image qualities and accommodative responses for different viewing conditions and system setups, and further decide the optimal view sampling guidelines for engineering LF-3D displays. It is worth mentioning that the methods and results can be generally applicable to both the emerging head-mounted LF-3D displays and the better-established eyewear-free direct-view LF-3D displays.
2. Generalized model and image formation process of light field displays
Several different methods have been explored to implement LF-3D displays, including SMV displays [11–15], integral-imaging (InI) based displays [16–18], and computational multi- layer light field displays [19,20]. Analogous to the camera array method for light field capture, an SMV display generally employs an array of 2D displays, each of which renders an elemental view of a 3D scene from a given viewpoint, to produce dense samples of the light rays apparently emitted by the scene. For instance, Lee et al. demonstrated the construction of a 100-inch, 300-Mpixel, and horizontal-parallax only light field display system by using 304 projectors arranged in a 19 by 16 array with about 0.17° horizontal parallax interval [14]. Instead of using an array of displays, alternatively, similar light field displays can be implemented in a time-multiplexed fashion. Jones et al. demonstrated a 360° horizontal- parallax light field display system by projecting light-field patterns onto a rapidly spinning anisotropic reflective surface through a single high-speed projector [15]. Most of the existing systems based on SMV-like method, however, only render the horizontal parallax of the light field due to the enormously increased complexity if vertical parallax would be considered. An InI-based display, utilizing the same principle as integral photography technique invented by Lipmamn in 1908 [24], typically consists of a display panel and a 2D array which can be a micro-lens array (MLA) [16,17] or aperture array [18]. While the 2D array angularly samples the directional light rays of a 3D scene, the display renders a set of 2D elemental images, each of which represents a different perspective of the 3D scene through each MLA or aperture. The conical ray bundles emitted by the corresponding pixels in the elemental images intersect and integrally create the perception of a 3D scene that appears to emit light and occupy the 3D space. An InI-based display using 2D arrays allows the reconstruction of a 3D shape with full-parallax information in both horizontal and vertical directions. A computational multi- layer display is a relatively new, emerging class of LF-3D display method that samples the directional rays through multi-layers of pixel arrays. It typically employs a stack of light- attenuating layers illuminated by a backlight [19]. The light field of a 3D scene is computationally decomposed into a number of masks representing the transmittance of each layer of the light attenuators. The intensity value of each light ray entering the eye from the backlight is the product of the pixel values of the attenuation layers at which the ray intersects. The computational multi-layer displays operate in a multiplicative fashion and approximate the light fields of the scene by angularly sampling the directions of the light rays apparently emitted by a 3D scene in a fashion similar to the InI-based display. The pinlight display, consisting of a spatial light modulator (SLM) and an array of point light sources, namely the pinlights, may also be considered as a computational multi-layer display where the back layer of light attenuators next to the backlight is replaced by a pinhole array or an array of point sources [20].
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18511

 Although the various methods for implementing LF-3D displays summarized above appear to largely differ from each other, they all share the common characteristics of reproducing the directional light rays apparently emitted by a 3D object either in horizontal direction only or in both horizontal and vertical directions. Without loss of generality, we therefore simplify the image formation process of a LF-3D display into a generalized model adapted from the well-known 4-D light field function, L(s, t, u, v), for representing the ray radiance as a function of positions (s, t) and directions (u, v). As illustrated in Fig. 2(a), the generalized model can be divided into two subparts: a light field engine (in blue box) and an eye model (in red box).
Fig. 2. The generalized schematic model of a light field 3D display, along with the illustration of the retinal response of a reconstructed point when the accommodative distance, A, of the eye is (a) the same as, (b) larger than, or (c) smaller than the depth of the reconstructed point. (d) The schematic drawing of the eye model entrance pupil and footprints of elemental views on the viewing window.
The light field engine minimally consists of a rendering plane, a modulation plane, a central depth plane, a reconstruction plane, and a viewing window. The rendering plane is where the elemental images are displayed and may be considered as the source defining the positional information of the light field function. It is an abstract representation of an array of displays in SMV systems [11–15], or a single physical display [16] or its virtual image through an eyepiece [17,18] being spatially divided into multiple regions in the case of InI- based displays, or the modulation layer closest to the backlight in computational multi-layer displays [19], or the point source array in pinlight displays [20]. The elemental images function as spatially-incoherent objects with each representing a unique perspective of a 3D scene. The modulation plane is where the directional samples of the light rays are produced and may be considered as the component defining the directional information of the light field function. It is an abstract representation of an array of optics [11–15], a lenslet array [16,17], an aperture array [18], or SLM(s) [19,20] in the different LF-3D display methods above. In SMV and InI-based LF-3D displays, the modulation plane consists of an array of optical
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18512
 
 elements (e.g. lenses or apertures) each of which corresponds to an elemental image on the rendering plane and creates one directional sample of a 3D scene. In a computational multi- layer type displays, each pixel on the modulation plane may be considered as an element and defines one directional ray sample. The central depth plane (CDP) is viewed as a reference plane where the light rays created by a point source in the rendering plane converge after propagating through the modulation plane. It typically refers to the optical conjugate of the rendering plane through the modulation plane. It is where usually the highest spatial resolution of the reconstructed 3D scene can be obtained. Among the aforementioned three methods for LF-3D displays, both the SMV and InI-based displays have a well-defined CDP in the system. The computational multi-layer light field method, however, does not necessarily have a clearly optical conjugate plane in its system construction. To account for the diffraction effects of wave propagation through the layers of pixel structures, we can consider the plane located at the last surface of the SLM stack as its reference plane. The reconstruction plane is a representation of the depth location where a 3D point, P, is to be rendered through a LF-3D display. Its location clearly varies with the 3D point of interest. Unless the depth of the reconstruction scene coincides with that of the CDP, usually the spatial resolution of the reconstructed object will be degraded compared to that on the CDP. The last component of the light field engine is the viewing window, which defines the area with which a viewer observes the reconstructed 3D scene. It is commonly known as the eye box of the system in a head-mounted display or viewing zone in an eyewear-free direct view display. The ray bundle originated from a pixel on the rendering plane propagates through the modulating plane and projects a footprint on the viewing window resembling the geometric arrangement of the corresponding modulation elements on the modulating plane.
On the other hand, the eye model that simulates the optics properties of the human visual system is carefully placed so that its entrance pupil matches the location of the viewing window of the light field engine. We further assume that the entrance pupil area of the eye model is no larger than the viewing window area and thus determines the total number of elemental images or views being imaged at the retina. To better evaluate the eye’s response in viewing a LF-3D display, a schematic eye model which is capable of adjusting its optical properties according to the eye’s accommodation state is required.
Each pixel on the rendering plane is considered as an elemental point source and emits a conical ray rundle that propagates through an element on the modulation plane. As illustrated in Fig. 2(a), to reconstruct the light field of a 3D point, P, the rays emitted by the selected pixels located on different elemental images are directed by the corresponding elements on the modulation plane such that they intersect at the 3D position of reconstruction, or the reconstruction plane. The eye model then is anticipated to perceive the integral bundles of the rays originated from different pixels as if they were emitted by a source located at the point of reconstruction. The accommodated status of the eye model plays a critical role on the resulted retinal image. When the eye is accommodated at the depth of the reconstruction point, the retinal images of these spatially separated pixels will overlap with each other and form a focused image of the reconstruction point, as illustrated in Fig. 2(a). Otherwise, the retinal images of the individual pixels will be spatially displaced from each other, as illustrated in Figs. 2(b) and 2(C), and integrally create a retinal blur and the level of the retinal blur varies with the difference between the depths of the reconstruction and eye accommodation. In other words, the accommodation state of the eye model will not only change the perceived retinal image of the individual elemental pixel, but also the displacement between them. Under such circumstance, the overall retinal image of the reconstructed point shall be considered as an integral of the retinal images of individual elemental pixels with corresponding displacements. The retinal image quality apparently is expected to vary when the eye is accommodated at different depth and the resulted eye accommodation response to a reconstructed depth depends on the focus cues rendered by the display.
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18513

 Based on the principle of light field reconstruction described above, we can characterize the retinal image properties of the reconstructed light field by its normalized, accumulated point spread function (PSF), PSFLF-Accu, by integrating the PSFs of the retinal images of all the elemental pixels, PSFc, assuming incoherent condition which almost all the LF-3D displays satisfy. For the convenience of characterizing the imaging properties of a LF-3D display and eye accommodative response from the point of view of a viewer, the center of the viewing window is set as the origin of the coordinate system OXYZ, the Z-axis is along the viewing direction, and the OXY plane is parallel to the viewing window. The depth of the CDP with respect to the eye is denoted as ZCDP, and the depth of the eye accommodative state is denoted as A. The coordinates of an arbitrary point of construction, P, are defined as P(x,y,z) while its depth is more conveniently defined by its axial displacement from the CDP, Δz. A positive Δz indicates the point of reconstruction is further away from the viewer than the CDP. We further define a reference frame, O’X’Y’, for the retinal image plane, with the origin O’ located at a distance, Zeye, which is the effective distance from the retinal plane to the eye pupil.
The view sampling property of a LF-3D display is characterized by the view density, σview, defined as the number of views per unit area on the entrance pupil of the eye. The total number of elemental views entering the eye pupil can be determined by checking the number of elemental views encircled inside the eye pupil and can be explicitly expressed as
πD2σ
Nview = 4 view , (1)
where D is the entrance pupil diameter (EPD) of the eye. For instance, assuming a 3-mm EPD, a view density of 0.142mm−2 corresponds to only one view across the entire eye pupil, or a density of 1.274mm−2 corresponds to 9 view samples across the eye pupil.
The footprint of an individual view projected on the viewing window and the distribution of all the elemental views on the viewing window depend on the shape and arrangement of the modulation elements on the modulation plane, as shown in Fig. 2(d). For simplicity, we assume the elemental views are evenly distributed on the eye pupil in a rectangular array symmetric to the optical axis (z-axis), and their footprints are perfectly circular. Under such assumptions, the lateral displacements between two adjacent elemental views on the viewing window, Δdx and Δdy, along the x- and y-directions, respectively, are given by
1
πσ view
Rfp=aπσ , (3)
view
. (2) The footprint size of an elemental view projected on the viewing window is characterized
by its radial distance, Rfp, given as
∆d ∆= d= 2 xy
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18514
  1
 where a is a scalar factor between 0 and 1 defining the fill factor of each elemental view. A fill factor of 1 suggests the footprint of each elemental view is the same as the pitch of the elemental views, while a fill factor less than 1 suggests that the footprint of each elemental view is smaller than the view pitch.
Based on the model described above, the elemental view of a LF-3D display with a view density greater than 0.142mm−2 typically under-fills a 3-mm eye pupil, allowing more than one view to be perceived simultaneously. For convenience, we assume the number of views entering the eye pupil is an integer rounded from the Eq. (1) so that complete sets of elemental views could be perceived. The accumulated PSF of the perceived light field on the retina, PSFLF-Accu, can be therefore expressed as

 PSFLF−Accu(x',y')= M N K
where M and N are the total number of elemental views entering the eye pupil along the X and Y directions, respectively; K is the number of the sampled wavelengths, and PSFCmnk is the retinal PSF of a given elemental view indexed as (m,n) at a given wavelength, λk. In a LF- 3D display, the weights of different elemental views to the accumulated retinal image may vary slightly to reproduce the subtle luminance differences of the object when viewed from slightly different positions. We thus introduce L, which may be regarded as the normalized luminance value of an elemental view indexed as (m,n) received by the eye from the point of reconstruction, to account for the weights of different elemental views to the accumulated PSF. w is the weighting function applied to the retinal PSF of an elemental view for the kth sampled wavelengths, λk, accounting for the relative luminous response of the human visual system to different illumination sources, and s is another weighting function applied to the PSF of a given elemental view indexed as (m,n) depending on its entry position, and dcxm and dcyn, on the eye pupil, to account for the directional sensitivity of the photoreceptors on the retina, known as the Stiles-Crawford effect [25]. For an elemental view indexed as (m,n), its entry position on the eye pupil, (dcxm, dcyn), is given by
dcxm =(2m−M−1) ∆dx; dcyn =(2m−M−1) ∆dy. (5) 22
The exact form of the monochromatic, coherent, retinal PSF of an individual elemental view, PSFc, through the combined model depends on both the light field engine and eye model adopted and is better computed numerically. Assuming that the light field engine is diffraction-limited (aberration-free) and shift-invariant, the retinal PSF of an individual elemental view can be modeled as

∑∑∑ m=1n1=k1 =
k xm yn Cmnk
MNK
m=1n1=k1 =
L(m,n)w(λ)s(dc ,dc )PSF (x',y') ,
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18515
1 ∑∑∑L(m,n)w(λ )s(dc ,dc )
 k xm ym
(4) 2
  1ππ PSF(x',y')=λz z exp jλz ∆x'+∆y' exp jλz x'+y'
)  ( ) C2(2222
cc
CDP eye  CDP   eye 
  ∞ 2π π112 2
•∫∫P (x,y)exp j W (∆x ',∆y ';x,y) exp j −
−∞
where Δxc’ and Δyc’ are the lateral displacements of an elemental view of a given reconstructed point from z-axis on the CDP, Peye is the footprint of each elemental view upon the pupil of the eye model, and Weye is the wavefront function standing for the amount of aberrations in the adopted eye model measured at the pupil, which may vary from model to model, and is influenced by various ocular factors such as accommodation state or EPD. Following the same assumption about view distribution as for Eqs. (2) and (3), Δxc’, Δyc’ of an on-axis reconstructed target point and Peye for an elemental view indexed as (m,n) can be further expressed by Eqs. (7) and (8), respectively, as,
(x +y ) (6) eye eyecc
λλzA CDP  
  2π∆x'x' ∆y'y'
•exp−j λ (z c + z )x+(z c + z )ydxdy,  CDP eye CDP eye 

' xm ' xm
∆x = dc ∆z ; y dy∆∆z=, (7)
cm zCDP +∆z cn zCDP +∆z
  
  MTF
LF − Accu
(ξ ',η ') = −∞
LF−Accu  
, (9)
P (x, y) = circ eye(m,n)
 (x−dc )2 +(y−dc )2  xm yn
 Rfp  
, (8)
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18516
 where circ is the circular function working as a binary sub-aperture window. Peye helps define the entry position for each of the elemental view at the entrance pupil of the eye model and thus determines which sub-part of the eye model will interact with the corresponding elemental view and changes the form of its corresponding PSFc calculated from Eq. (6).
Based on the image formation process above, the MTF of the perceived light field on retina, characterized by the ratio of the contrast modulation of the retinal image to that of a sinusoidal object rendered on the display, can be obtained by applying Fourier transform to Eq. (4), which is expressed as
∞
∫ ∫ PSF (x ', y ')exp − j (ξ ' x '+η ' y ') dx ' dy '
 NAmod = Z πσ CDP
view
∞
∫∫PSFLF−Accu (x',y')dx'dy'
−∞
where ξ' and η' are the spatial frequencies measured on the retina in the x’- and y’-directions, respectively.
It is worth pointing out that the accumulated PSF and MTF of the perceived light field on retina, PSFLF-Accu and MTFLF-Accu, calculated by Eqs. (4) and (9), respectively, only depend on the CDP location (ZCDP), the view density of light field sampling (σview), the fill factor of each elemental view (a), the axial displacement of the reconstruction depth from the CDP (Δz), the eye pupil diameter (D), the spectral property of the source (λ), the eye accommodation status (A), as well as the wavefront function of the adopted eye model (Weye). The depths of the rendering plane and the modulation plane have no influence upon the retinal image, however, the numerical aperture (NA) of the modulating elements, NAmod, on the modulation plane is required to match the view density σview or footprint size Rfp such that
2Rfp
= Z . (10)
CDP
2a 1
 In summary, the generalized model of a LF-3D display described above enables us to quantify the perceived retinal image quality of a LF-3D display by an observer, assess whether the eye would properly accommodate at the depth of a 3D reconstruction, quantify the accuracy of the resulted focus cues when viewing a LF-3D display, and investigate how key system design parameters such as view density and fill factor of each elemental view will influence the retinal response in viewing LF-3D displays.
3. Characterizing the perceived retinal images of a light field display
Equation (4) demonstrates that the overall retinal response of the eye viewing a LF-3D display is actually the integral of the weighted PSFs of all the elemental views reconstructing a 3D point, which suggests that there exist fundamental differences between the reconstructed 3D scene by LF-3D displays and a natural 3D scene, or a 2D scene (a displaying plane) by traditional displays. Assuming that the accommodative depth of the eye, A, coincides with the reconstruction depth, this section will focus on characterizing the perceived retinal image quality of a LF-3D display.
To simulate and characterize the perceived retinal image of a LF-3D display based on the generalized model in Fig. 2, we created a multi-configuration zoom system in Code V® [26], in which each configuration represents the path of an elemental view perceived by a shared eye model. For simplicity and without loss of generality, the light field engine was

 implemented using the InI-based method similar to the one described in [17]. The rendering plane was modeled by fiducial display with infinite pixel resolution and the modulation plane was assumed to be an aberration-free MLA. A circular aperture was assumed for each element of the MLA. The lenslet pitch and NA of the MLA were set to match the view density and fill factor of interest given by Eqs. (2) and (10), respectively. The microdisplay in the model was set up with five wavelengths, 470nm, 510 nm, 555 nm, 610 nm, 650 nm, respectively, to simulate a full-color LF-3D display, of which the relative weights were set according to human eye’s photopic response curve [25] to reflect the polychromatic responses in Eq. (4). With this simplified construction of a light field engine, the CDP was the optical conjugate of the microdisplay and the PSF of each elemental view through the lenslet was simplified to be diffractive-limited. Reconstructed 3D targets for LF-3D displays were also assumed on or near z-axis. In addition, a schematic eye model capable of varying its optical properties according to the eye’s accommodation state was required to complete the model for perceived retinal image assessment. Several schematic eye models have been widely used in the fields of visual and ophthalmic optics to predict the performance of an optical system involved with human observers [25, 27]. Among the various eye models, the Arizona eye model was selected, which was designed to match clinical levels of aberrations for both on- and off-axis fields and provides the capability of changing the accommodative distance, A, of the eye by varying the shape and refractive index of the crystalline lens [25]. The parameters and dimensions of the Arizona eye model have been chosen to be consistent with the population-average data so that the conclusions resulted from such a model could be applied to the majority of the potential viewers. The eye model optics was integrated into the multi- configuration zoom system in Code V. The entrance pupil of the eye model was set at 1 diopter (1m) away from the CDP of the light field engine. The accommodative distance of the eye, which determines the lens shape, conic constant and refractive index of the surfaces in the schematic eye, can be varied as needed. We set the entrance pupil diameter of the eye model to be 3mm, which corresponds to the average pupil size when viewing typical displays with luminance around 200 cd/m2. In order to account for Stiles-Crawford effect [25], a Gaussian apodization filter with an amplitude transmittance coefficient of β = −0.116mm−2 was selected as the weighting function s in Eq. (4). Hence the elemental view that passes through the central part of the eye pupil would have a larger contribution to the accumulated PSF than a view through the edge of the eye pupil.
Based on the setup described above, for a given display configuration, the depth of a reconstruction point, the accommodation state of the eye, and the pixel locations of the elemental views on the rendering plane to reconstruct the point of interest are determined. The retinal PSF of each elemental view given by Eq. (6), can be simulated independently using CODE V, and the accumulated PSF of the retinal image of such a reconstructed point given by Eq. (4) was obtained by integrating the retinal PSFs of all the elemental views passing through the eye pupil, and the corresponding MTF of the perceived LF-3D display was computed using Eq. (9).
We began with investigating how the spatial resolution of a LF-3D display is affected by the view density and the depth of reconstruction. Figure 3(a) plotted the polychromatic MTF of the retinal image as a function of spatial frequencies for reconstructed 3D targets located on the CDP for LF-3D displays with different view densities ranging from 0.142 to 2.26mm−2, corresponding to footprint pitch from 3mm to 0.75mm. In this simulation, the fill factor of the elemental views was set to be 1 (i.e. a = 1) and the accommodative distance, A, of the eye model was assumed to coincide with the depth of the CDP. The MTF plot for the view density of 0.142mm−2 is equivalent to the MTF when viewing a traditional 2D display placed at the same depth as the CDP. Apparently, as the view density increases, the MTF of the perceived light field reconstruction at the depth of the CDP decreases rapidly due to the increasing effects of diffraction with small sub-apertures of elemental views. For instance, with a view density of 2.26mm−2, a total of 16 views were integrated to reconstruct a 3D point
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18517

 located on the CDP and the MTF of the retinal image drops down to zero at approximately 23 cycles/degree mainly due to diffraction effects by the small NA of the modulation elements.
Fig. 3. (a) The MTF plots of the retinal image of light field displays with different view densities ranging from 0.142 to 2.26mm−2 for reconstructed 3D targets located on the CDP. (b) The MTF plots of the retinal image of a light field display with a view density of 0.57mm−2 for reconstructed targets located at four different depths from the CDP: 0, 0.3, 0,6, and 0.9 diopter, respectively. (c) Simulation of the perceived retinal images of a series of Snellen “E” s of three different angular resolutions located at four different depths from the CDP. (d) The cut-off frequencies of light field display with different view densities as function of the depth displacement from the CDP.
Figure 3(b) plotted the polychromatic MTF of the retinal image as a function of spatial frequencies for reconstructed 3D targets located at four different depths from the CDP: 0, 0.3, 0.6, and 0.9 diopters, respectively. All the displacements were on the close side of the CDP with respect to the eye. The accommodative distance, A, of the eye model was varied so that it coincided with the depth of the reconstructed targets: 1, 1.3, 1.6, and 1.9 diopters, respectively. The view density was assumed to be 0.57mm−2 with a fill factor of 1, corresponding to a view pitch of 1.5mm, which yields a total of 4 views over the eye pupil. It is clear that the perceived MTF of such a LF-3D display decreases rapidly as the reconstruction point is displaced away from the CDP, which correlates to a degradation of spatial resolution. By setting a minimal threshold value for the MTF of a LF-3D display, for example, 0.1 as shown by the red dotted line in Fig. 3(b), the cut-off frequencies, or the limiting spatial resolution, for reconstructed 3D points at different depths can be determined. For instance, with the given view density of 0.57mm−2 and a minimum MTF threshold of 0.1, the cut-off frequencies for the reconstructed scene are approximately 34, 33, 30, and 24 cycles/degree for the depth displacement of 0, 0.3, 0.6, and 0.9 diopters from the CDP, respectively. Figure 3(c) shows the simulation of the perceived retinal images by convolving a series of Snellen ‘E’s of three different angular resolutions (10, 20 and 30 cycles/degree) with the accumulated PSF corresponding to of the MTF curves as in Fig. 3(b). Retinal image
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18518
 
 quality degradation can be clearly observed as the reconstruction depths of the targets are displaced away from the CDP.
Using the same MTF threshold of 0.1, Fig. 3(d) plotted the cut-off frequencies of a LF-3D display as a function of dioptric depth displacement from the CDP for different view densities ranging from 0.142 to 2.26mm−2. For a given view density, the cut-off frequency generally decreases as the depth displacement increases. As the view density increases, the cut-off- frequencies on the CDP, which corresponds to the maximum constructible frequency, decrease rapidly due to the increasing effects of diffraction with small sub-apertures of elemental views. On the other hand, as the view density increases, the spatial resolution degradation, characterized by the slope of the curves, becomes less sensitive to depth displacement from the CDP, which can be explained by the increasing DOF of each elemental view with a decreasing sub-aperture. The cut-off frequencies become nearly independent of the depth displacements when the view density is about 2.26mm−2 or greater.
Based on the results shown in Fig. 3, the DOF of a LF-3D display can be defined as the depth range within which a given spatial resolution criteria can be achieved. For instance, the DOF of a LF-3D display constructed with a view density of 0.57mmmm−2 is approximately ± 0.65 diopters to achieve a minimal spatial resolution of 35 cycles/degree, while it is about ± 1 diopters to achieve a minimal spatial resolution of 25 cycles/degree. Although a larger view density offers a larger DOF for the same resolution criteria, it comes at the cost of lower image resolution on the CDP and also lower image contrast. A lower view density offers higher image resolution on the CDP, but compromises the DOF and potentially larger accommodation error, which to be further discussed.
4. Characterizing the accommodative response of a 3D light field display
The focusing cues rendered by a LF-3D display can be characterized by two components. The first part is the cue rendered by the reconstruction depth at which the rays of the elemental views appear to converge, namely the accommodation cue. The other is the blur cue rendered by the perceived retina image which stimulates the eye to change its accommodation response to maximize retinal image sharpness. The retinal image blur is the sole true stimulus to accommodation and is the visual information used to effect predictable changes in the accommodative response. A light field reconstruction presumably generates the elemental views such that the accommodation cue is correctly rendered. The actual accommodative response, however, is affected by the actual retinal image blur rendered by a LF-3D display which may possibly drive the eye to accommodate on neither the CDP nor the depth of the reconstructed point, but somewhere in between to balance between the two aforementioned components of focusing cues. The mismatch between the actual accommodative response of a viewer to a LF-3D display and the accommodation cue rendered by the images is considered as accommodation cue error which potentially leads to residual VAC problem. One of the main objectives of this section is to characterize the accommodative response and quantify the accommodation cue error rendered by a LF-3D display with respect to key systematic parameters.
Generally, the contrast gradient and contrast magnitude of the retinal image are the key factors that drive and stabilize eye accommodation response. The eye tends to adjust its accommodation to maximize these two factors in the focusing process. Through the MTF in Eq. (9), it becomes relatively straightforward to quantify the contrast magnitude and gradient of the retinal image with respect to eye accommodation status and to characterize the eye accommodative response to a LF-3D display. Based on the same setup described in Sect. 3, the change of retinal image properties and the accommodative response of a LF-3D display can be characterized by varying the accommodation status of the eye model through the depth of interest. The actual accommodative distance, Am, that offers the maximum image contrast and gradient is located and is considered to be depth at which the eye is likely to
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18519

 accommodate. The image contrast and gradient resulted from the actually accommodative distance are considered to be focus cues rendered by the LF-3D displays.
To investigate the accommodative response of the eye to a LF-3D display, Fig. 4(a) through 4(c) plotted the MTF of the retinal image of a LF-3D display as a function of eye accommodation shift for reconstructed targets located at three different depths away from the CDP by a dioptric displacement of 0, 0.5, and 1 diopters, respectively. All the displacements of the reconstructed depths were on the same and the close side of the CDP with respect to the eye. Each figure plotted the responses to targets of 5 different spatial frequencies ranging from 5 to 25 cycles per degree (cpd) at a 5 cycles/degree increment. The sampled frequencies were selected based on the guidance provided by Fig. 3(c) for the cut-off frequencies. The CDP of the light field engine was set to be 1 diopter (1m) away from the eye as well as the view density to be 0.57mm−2 on the eye pupil, which corresponds to 4 different views passing through the area of a 3-mm eye pupil. The horizontal axis of the plots, denoted as the accommodation shift in diopters, is defined as the dioptric displacement of the eye accommodative distance, A, from the corresponding depth of the reconstruction target. A negative accommodation shift suggests that the eye accommodative distance is shifted closer toward to the CDP. The vertical axis is the polychromatic MTF value of the retinal image for a given eye accommodative distance. The black arrows marked the location where maximum retinal image contrast is achieved for each of the target frequencies. As a comparison, we further examined the MTF curves of the retinal images of real targets placed at 1, 1.5 and 2 diopters away from the eye, as a function of the eye accommodative distance for the same frequency range. The depths of these real retargets correspond to the same depths of the reconstruction planes in Fig. 4(a)-4(c). The same eye model was utilized. The only difference from the Fig. 3(a) is that the target is a real target, rather than being reconstructed by multiple elemental views of small NAs. As an example, Fig. 4(d) plotted the MTF curves of the retinal image of a real target, placed at the same depth as the CDP, as a function of the eye accommodative distance. It is worth pointing out that the accommodative responses of the eye to real targets of other depths are very similar to the one shown in Fig. 4(d) and are omitted without redundancy.
When the reconstructed target is located on the CDP (Δz = 0 diopter) as shown in Fig. 4(a), it can be observed that the MTF values at different frequencies reach their maximums when the eye is accommodated at the reconstruction depth but gradually decrease as the accommodation shift increases on either side of the reconstruction plane. Across the entire frequency range of simulation, the contrast gradient, which primarily drives the eye to accommodate at the proper depth, of the retinal image of the reconstructed light field on the CDP is lower than that of a natural target shown in Fig. 4(d) but with similar trend. However, at any given eye accommodative distance, the image contrast of the reconstructed light field was noticeably lower than that of a natural target for all the frequencies and the contrast degradation is more prominent in the high frequency range. For instance, the maximum contrast for the frequency of 25 cycles/degree is about 0.2 for the reconstructed target while it is over 0.35 for a real target. Such noticeable difference in image contrast can be explained by the diffraction effects due to the substantially smaller NA of each elemental view than that of a real object to the eye pupil as a whole. Overall, Fig. 4(a) indicates that a reconstructed target on or near the CDP by a LF-3D display can provide correct focus cues, comparable to a natural target.
As the displacement of the reconstruction depth from the CDP significantly increases, as shown in Figs. 4(b) and 4(c), however, the focus cues rendered by a LF-3D display may not be in agreement with the depth of rendering. It can be clearly observed that both the contrast magnitude and gradient of the retinal image are noticeably lower, especially in the 10-20 cycles/degree frequency range. Furthermore, the peak responses are shifted away from depth of the reconstruction. Instead, they are shifted towards the location of CDP, leading to a negative accommodation shift. For instance, as shown in Fig. 4(c) with 1 diopter
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18520

 displacement from the CDP, the accommodative distance corresponding to the maximum image contrast was shifted away from the depth of reconstruction by as large as about 0.2 diopters for targets of 5 cycles/degree or over 0.05 diopters for targets of 15 cycles/degree. This indicates that the focus cues rendered by a LF-3D display for 3D targets located far from the CDP bear a significant error compared to their rendering depth, which limits the ability to address the VAC problem.
Fig. 4. (a)-(d) The accommodative response curves for different spatial frequencies (cpd stands for cycles per degree) with the reconstruction plane located at (a) Δz = 0 diopter, (b) Δz = 0.5 diopters, (c) Δz = 1 diopter, and (d) with a real target located on the CDP. (e)-(f) Accommodation error for different spatial frequencies as a function of the dioptric displacement of a target depth from the CDP toward the eye for (e) targets reconstructed by a LF-3D display and (f) real targets.
To better illustrate the accuracy of focus cues rendered by of a LF-3D display in relation to the depth of reconstruction, the axial displacement between the actual accommodative distance, Am, and the reconstruction distance is defined as accommodation error ΔA. Am corresponds to the locations marked by the black arrows in Figs. 4(a)-4(d) and is the depth where the eye will practically accommodate, while the reconstruction distance is where the eye theoretically shall accommodate. Along with the contrast magnitude and gradient of the resulted retinal image, ΔA provides an objective measurement on whether a given display configuration is able to provide accurate and distinctive focus cues. Figure 4(e) plotted the
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18521
 
 accommodation error as a function of the dioptric displacement of a reconstructed target from the CDP for the spatial frequencies ranging from 5 to 25cycles/degree. The depths of the reconstructed targets for simulation were shifted away from the CDP by a magnitude up to 3 diopters at a 0.5 diopter increment toward the eye, corresponding to the reconstruction depth range from 1 to 3.5 diopters away from the eye. As the reconstructed target is displaced away from the CDP by more than 1 diopter, the 3D-LF display configuration used in the simulation is unable to reconstruct targets of high spatial frequencies, which was suggested by the cut-off frequencies in Fig. 3. Therefore Fig. 4(e) only plotted accommodation errors for targets within its corresponding reconstructable depth range for a given frequency. As a comparison, Fig. 4(f) plotted the accommodation error for real targets placed at the same dioptric depths. It is worth noting that the curves for all of the spatial frequencies overlap with each other due to the negligible errors for real targets. The results clearly suggest that the accommodation error increases noticeably as the increase of the dioptric displacement from the CDP for targets rendered by a LF-3D display, while the accommodation errors for real targets are negligible as expected. Although the plots in Fig. 4(e) only considered positive dioptric displacement of a reconstructed target from the CDP (i.e. moving closer to the eye from the CDP), we anticipate nearly symmetric performance for negative displacements from the CDP (i.e. moving further away from the CDP).
Because the relatively low frequency content owns better image contrast in general, there always exists a trade-off between focus cue accuracy and retinal image quality of a light field system. According to the neural transfer function (NSF), which is the contrast of the effective neural image divided by retinal-image contrast as a function of spatial frequency, human eye is most sensitive to the mid-range frequency around 10-15 cycles/degree [28]. This also agrees with the Fig. 4(d), where the MTF curve of the mid-range frequencies has the largest gradient that will effectively drive the eye accommodation response toward the depth which yields maximal image contrast. Therefore, in the following section, we will target on the mid- range frequencies to investigate how we can further reduce the accommodation error by optimizing the system parameters of LF-3D displays.
5. Optimal view sampling of light-field displays
As discussed in Sections 3 and 4, the spatial resolution of a LF-3D display and the accuracy of the focus cues rendered by the display not only vary greatly with the displacement of the reconstruction depth of a 3D scene but also with the view density of the display design. This section aims to investigate the optimal view sampling for LF-3D display designs.
The accommodation cue of a LF-3D display originates from the intersection of multiple elemental views at the depth of reconstruction. Therefore, the number of views that fill the eye pupil or the view density plays a key role in influencing not only the spatial resolution and DOF of a LF-3D display but also the accuracy of focus cues rendered the display to drive proper eye accommodative response. In viewing a natural scene of different depths in the real word, the eye observes infinite number of views entering the eye pupil from the scene, which yields subtle yet accurate focus cues to drive eye accommodative responses. Naturally, in designing a LF-3D displays, the more the number of views or the higher the view density with which we sample the light field to be rendered, the less the accommodation error and thus the more accurate the accommodation cue may be anticipated. Due to the diffraction effect, however, a higher view density may lead to a lower spatial resolution overall. It is of great importance to balance between the accommodation cue and spatial resolution, and therefore the number of views or view density needs to be optimized.
Figures 5(a) and 5(b) plotted the accommodative response of LF-3D displays with different view densities ranging from 0.57 to 2.26mm−2, corresponding to sub-aperture diameter from 1.5mm to 0.75mm (or 4 to 16 views across the eye pupil). The fill factor of the elemental views was assumed to be 1. The CDP of the display was fixed at 1 diopter (1m) and the diameter of the entrance pupil of the eye was set as 3mm. The reconstruction depth of the
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18522

 target was set on the CDP and 1 diopter away from the CDP for Figs. 5(a) and 5(b), respectively, while both figures plotted the MTF values for the mid-range frequency of 15 cycles/degree.
Fig. 5. The accommodative response curves for different view densities with the reconstruction plane at (a) Δz = 0 diopter and (b) Δz = 1 diopter. The target frequency was set at 15 cycles/degree.
When the reconstructed target is located on the CDP (Δz = 0 diopter) as shown in Fig. 5(a), the MTF values in all of the three sampled view densities reach their maximum when the eye is accommodated at the reconstruction depth. As expected, the contrast magnitude and gradient decrease noticeably as the view density increases due to increasing diffraction effects. Overall, all the sampled view densities can provide adequate retinal image contrast and gradient to stimulate correct eye accommodation. When the target reconstruction depth is displaced by 1 diopter from the CDP as shown in Fig. 5(b), however, noticeable accommodation error is observed for displays with low view density or small number of views. As the view density increases, the accommodation error is noticeably reduced. The magnitude and gradient of the image contrast degrade just slightly as the increase of view density. For instance, in a display configuration with a view density of 0.57mm−2, the accommodation error is about 0.05 diopters for targets rendered at 1 diopter away from the CDP. As the displacement from the CDP increases, the magnitude of the accommodation error, the image contrast and contrast gradient degrade dramatically. In a display configuration with a view density of 1.27mm−2, the accommodation error of mid-range frequency content is nearly zero for targets rendered on the CDP and for targets rendered 1 diopter away from the CDP, while the magnitude and gradient of the image contrast are slightly worse than those for a display configuration with a view density of 0.57mm−2. However, the advantage of creating a high view density ceased when diffraction effects dominate. For example, as shown in Fig. 5, a display configuration with a view density of 2.26mm−2 lead to noticeably worse image contrast and contrast gradient than a configuration with a view density of 1.27mm−2 for targets located on the CDP and 1 diopter away from the CDP. Therefore, the accommodation cue rendered by such a display is not necessarily improved at the cost of spatial resolution.
Figures 6(a)-6(c) plotted the accommodation error, the maximum contrast magnitude, and maximum contrast gradient of the retinal image, respectively, as a function of the axial displacement of the reconstruction depth from the CDP for different view densities from 0.57 to 2.26mm−2. On each figure, the results for targets rendered at four different depths: 0, 033, 0.67, and 1 diopter from the CDP, respectively, were plotted. The red dotted lines in the plots stand for the accommodative curve of viewing an ideal real target. The simulation setup remained the same as those in Fig. 5. The results in this figure can be utilized as a general guideline when selecting the appropriate view density for designing a LF-3D display system. What is desired is a LF-3D display system that generates no accommodation error and also a
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18523
 
 high, constant contrast contents along a large DOF. There obviously exists trade-off between these two factors.
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18524
 Fig. 6. The plots of (a) accommodation error, (b) the maximum contrast magnitude and (c) maximum contrast gradient for Δz from 0 to 1 diopter with varying view densities.
All the simulations above assume a fill factor of 1 for the elemental views. In fact, the fill factor, a, of the elemental views defined in Eq. (3) can have significant influence on the perceived light field image. Various methods [29, 30] that involve in adjusting the effective fill factor have been proposed to improve different aspects of the LF-3D display systems. Changing the fill factor will directly redefine the effective working NA of element views (Eq. (10)) and thus dramatically impact the integral light field image. Reducing the fill factor of elemental views is considered as being a simple, straightforward way to improve the accuracy of the accommodation cue, especially for a system with a low view density at the cost of reduced display brightness and potentially compromised spatial resolution.
 Fig. 7. The accommodative curves for displays with a view density of 0.57mm−2 and different fill factors ranging from 1 to 0.4 with the reconstruction plane at (a) Δz = 0 diopter, (b) Δz = 1diopter. The target frequency was set at 15 cycles/degree.
In order to investigate the effects of the fill factor of the modulation element, we slightly modified the model setup in CodeV by reducing the effective aperture size of each lens element of the MLA on the modulation plane while maintaining the same lens pitch by fixing the amount of decenter among the zooms. Figures 7(a) and 7(b) show the result of the accommodative response of display configurations with different fill factors ranging from 1 to 0.4. The CDP of the display was fixed at 1 diopter (1m), the diameter of the entrance pupil of the eye was set as 3mm and the view density was set to be 0.57mm−2 since under such circumstance, the accommodation error is noticeable as shown in Fig. 6. The reconstruction depth of the target was set on the CDP and 1 diopter away from the CDP, for Figs. 7(a) and 6(b), respectively, while both figures plotted the MTF values for the mid-range frequency of 15 cycles/degree.
Overall, the results in Fig. 7 suggest that reducing the fill factor of elemental views for displays of the same view density can noticeably reduce the accommodation error.

 Furthermore, for targets with large depth displacement from the CDP (Fig. 7(b)), properly picking the fill factor can also improve the magnitude and gradient of the retinal image by obtaining good balance between the diffraction and aberration effects. For instance, in a display configuration with a view density of 0.57mm−2, by reducing the fill factor to 0.6, the accommodation error for targets rendered at 1 diopter away from the CDP can be reduced to nearly zero and the contrast magnitude and gradient of the retinal image are also noticeably improved. In this case, it might be advantageous to adopt a fill factor of 0.6 that offers negligible accommodation error and high image contrast for a depth range of ± 1 diopters from the CDP. For a display configuration with a view density of 1.27mm−2 or more, reducing the fill factor mainly improves the accommodative error for targets with large displacement from the CDP, but have little improvements or even adverse impacts on the image contrast and contrast gradient since the diffraction effect starts to dominate and thus does not show noticeable benefits.
6. Conclusion
We describe a generalized framework to model the image formation process of light field display methods and present a systematic method to simulate and characterize the retinal image quality and accommodation response rendered by a LF-3D display. We further employ this framework to investigate the trade-offs and guidelines for optimal view sampling in the design of LF-3D displays. By taking both ocular and display factors into account, we determine that increasing the view density generally leads an increase of DOF and a reduction of accommodation error at the cost of spatial resolution and image contrast. The maximally achievable spatial resolution decreases with the increase of view density until the system reaches diffraction domination at a view density of 1.27mm−2, corresponding to 3 by 3 views across the eye pupil. At a view density of 1.27mm−2 or higher, the obtainable spatial resolution becomes nearly independent of the depth displacements, but the corresponding image contrast may be significantly compromised compared to a display of lower view density. For displays that can only afford lower view density such as 0.57mm−2 or lower, the fill factor of the elemental views may be reduced properly to improve the magnitude and gradient of the retinal image and reduce the accommodation error. Overall, the paper provides a framework that can objectively predict the performance and guide the design of LF-3D displays by optimizing the key parameters. In the future, we plan to build a high quality, compact LF-3D display and carry out further experiments to validate our methods.
Funding
This research was partially funded by National Science Foundation grant award 14-22653 and Google Faculty Research Award.
Disclaimer
Dr. Hong Hua has a disclosed financial interest in Magic Leap Inc. The terms of this arrangement have been properly disclosed to The University of Arizona and reviewed by the Institutional Review Committee in accordance with its conflict of interest policies.
Vol. 25, No. 16 | 7 Aug 2017 | OPTICS EXPRESS 18525
  See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/319854010
High dynamic range head mounted display based on dual-layer spatial
modulation
Article in Optics Express · September 2017 DOI: 10.1364/OE.25.023320
  CITATIONS
11
2 authors:
Miaomiao Xu
Facebook
17 PUBLICATIONS SEE PROFILE
READS
70
  102 CITATIONS
Hong Hua
The University of Arizona
175 PUBLICATIONS 3,069 CITATIONS SEE PROFILE
  Some of the authors of this publication are also working on these related projects:
Computational light field display View project Connected components, Eye tracking View project
   All content following this page was uploaded by Miaomiao Xu on 16 April 2020. The user has requested enhancement of the downloaded file.
 
 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23320
High dynamic range head mounted display based on dual-layer spatial modulation
MIAOMIAO XU AND HONG HUA*
3D Visualization and Imaging Systems Laboratory, College of Optical Sciences, University of Arizona, 1630 East University Boulevard, Tucson, AZ 85721, USA
*hhua@optics.arizona.edu
Abstract: The human visual system can perceive a much wider range of contrast than the addressable dynamic range afforded by the state-of-art head mounted displays (HMD). Increasing the dynamic range of HMDs becomes critical especially for augmented reality applications where the dynamic range of outdoor scenes can be as large as 14 orders of magnitude. In this paper, we present the integrated work of the design, implementation, calibration, and image-rendering algorithm of a high dynamic range HMD system. By using a pair of LCoS microdisplays as the spatial light modulators, accompanied with the relay optics to optically overlay the modulation layers, we demonstrate the reconstruction of high dynamic range images with high accuracy.
© 2017 Optical Society of America
OCIS codes: (120.2040) Displays; (120.2820) Heads-up displays; (120.4570) Optical design of instruments.
References and links
1. D. Gerwin, H. Seetzen, G. Ward, W. Heidrich, and L. Whitehead, “3.2: High dynamic range projection systems,” SID Symposium Digest of Technical Papers 38(1), 4–7 (2007).
2. B. Hoefflinger, High-Dynamic-Range (HDR) Vision (Springer 2007).
3. H. Seetzen, W. Heidrich, W. Stuerzlinger, G. Ward, L. Whitehead, M. Trentacoste, A. Ghosh, and A.
Vorozcovs, “High dynamic range display systems,” ACM Trans. Graph. 23(3), 760–768 (2004).
4. G. Wetzstein, D. Lanman, M. Hirsch, and R. Raskar, “Tensor displays: compressive light field synthesis using
multilayer displays with directional backlighting,” ACM Trans. Graph. 31(4), 80 (2012).
5. G. Wetzstein, D. Lanman, W. Heidrich, and R. Raskar, “Layered 3D: tomographic image synthesis for
attenuation-based light field and high dynamic range displays,” ACM Trans. Graph. 30(4), 95 (2011).
6. M. Hirsch, G. Wetzstein, and R. Raskar, “A compressive light field projection system,” ACM Trans. Graph.
33(4), 58 (2014).
7. S. H. Lu and H. Hua, “Imaging properties of extended depth of field microscopy through single-shot focus
scanning,” Opt. Express 23(8), 10714–10731 (2015).
8. J. B. Sibarita, “Deconvolution microscopy,” Adv. Biochem. Eng. Biotechnol. 95, 201–243 (2005).
9. P. Nema, “Digital imaging and communications in medicine (DICOM) Part 14: Grayscale standard display
function” National Electrical Manufacturers Association, Rosslyn, VA (2000).
10. R. Hainich and O. Bimber, Displays: Fundamentals and Applications (CRC, 2016).
11. CITIZEN FINEDEVICE Co, LTD., “QuadVGA −1280×960, 0.40′′ diagonal, single chip FLCoS display,”
https://www.miyotadca.com/mdca_product/quadvga.
12. M. Xu and H. Hua, “46‐1: Dual‐layer High Dynamic Range Head Mounted Display,” SID Symposium Digest of
Technical Papers 48(1), 668–671 (2017).
13. Z. Zhang, “Flexible camera calibration by viewing a plane from unknown orientations,” in The Proceedings of
the Seventh IEEE International Conference on Computer Vision (IEEE, 1999) 1, pp. 666–673.
14. S. Lee and H. Hua, “A robust camera-based method for optical distortion calibration of head-mounted displays,”
J. Disp. Technol. 11(10), 845–853 (2015).
15. O. Faugeras, Three-dimensional Computer Vision: A Geometric Viewpoint (MIT, 1993).
1. Introduction
A head-mounted display (HMD) is one of the key enabling technologies for virtual reality (VR) and augmented reality (AR) systems and has been developed for a wide range of applications. For instance, a lightweight optical see-through HMD (OST-HMD) enables optical superposition of two-dimensional (2D) or three-dimensional (3D) digital information onto a user’s direct view of the physical world and maintains see-through vision to the real world. It is viewed as a transformative technology in the digital age, enabling new ways of
#303119 https://doi.org/10.1364/OE.25.023320 Journal © 2017 Received 26 Jul 2017; revised 10 Sep 2017; accepted 12 Sep 2017; published 14 Sep 2017
  
 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23321
accessing digital information essential to our daily life. In recent years, significant advancements have been made toward the development of high-performance HMD products and several HMD products are commercially deployed.
Despite the tremendous progresses, one key limitation of the state-of-the-art HMDs is its low dynamic range (LDR). The dynamic range (DR) of a real scene, which has infinite and continuous luminous levels, is commonly defined as the ratio between the brightest and the darkest luminance in the scene, or as a base-10 or base-2 logarithmic value of the ratio. A display, however, typically renders finite and discrete number of luminous levels, and its DR is commonly characterized by the maximum command level (CL) that a display can produce or the bit depth (BD) for each pixel (or each channel in the case of color displays). Most of the state-of-the-art color displays, including HMDs, are only capable of rendering images with an 8-bit depth per color channel or maximally 256 discrete intensity levels. Such low bit depth is far below the capability of accurately rendering the broad dynamic range of real- world scenes that can reach as high as 14 orders of magnitude and infinitely fine luminous levels. In the meanwhile, the perceivable luminance variation range of the human visual system is above 5 orders of magnitude without adaptation [1]. For immersive VR applications, the images of LDR HMDs lack the capability of rendering scenes with large contrast variations, which may result in loss of fine structural details, high image fidelity, and sense of immersion. For optical see-through AR applications, the virtual images displayed by LDR HMDs may appear to be washed out with highly compromised spatial details when merged with a real scene which likely contains a much wider dynamic range by several orders of magnitude. The most common method of displaying a high dynamic range (HDR) image on conventional LDR displays is to adopt a tone-mapping technique that compresses the HDR the image to fit the dynamic range of the LDR devices while maintaining the image integrity. Although a tone-mapping technique can make HDR images accessible through conventional displays of nominal dynamic range, it comes at the cost of reduced image contrast which is subject to the limit of the device dynamic range and it does not prevent the displayed images being washed out in an AR display. Therefore, developing hardware solutions to HDR-HMD technologies becomes essentially critical, especially for AR applications.
Several efforts have been made toward developing hardware solutions to HDR displays for direct-view type desktop applications. Perhaps the most straightforward method toward HDR displays is to increase the maximally displayable luminance level and increase the addressable bit-depth for each of the color channels of the display pixels. However, it requires high-amplitude, high-resolution drive circuits as well as light sources of high luminance, which is challenging to implement at affordable cost [2]. An alternative method is to combine two or more layers of spatial light modulators (SLM) to simultaneously control the pixel output values. Seetzen et al. proposed an HDR display method for direct-view desktop displays based on a dual-layer spatial light modulating scheme [3]. Different from conventional liquid-crystal displays (LCD) using a uniform backlight, they utilized a projector to provide spatially modulated light source for a transmissive LCD to achieve dual- layer modulation and achieve a 16-bit dynamic range with two 8-bit SLMs. They also demonstrated an alternative implementation of the dual-layer modulation scheme, in which an LED array, driven by spatially-varying electrical signals, is used to replace the projector unit and provides a spatially-varying light source to an LCD [3]. More recently, Wetzstein et al [4,5] and Hirsch et al [6] demonstrated the use of multi-layer multiplicative modulation and compressive light field factorization method for HDR displays.
In principle, the aforementioned multi-layer modulation scheme developed for direct-view desktop displays can be adopted to the design of an HDR-HMD system by directly stacking two or more miniature SLMs along with a backlight source and an eyepiece. In practice, however, the direct stacking method of multiple layers of SLMs suffers from several critical problems, which make it practically infeasible for HDR-HMD systems. In this paper, we present a systematic work of the design, implementation, calibration, and image-rendering of

 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23322
an HDR-HMD system by adapting the dual-layer spatial modulation method. The rest of the paper is organized as follows. We discuss the key challenges in adapting the dual-layer modulation scheme for HDR-HMD designs in Section 2, describe the overall optical system design and prototype implementation in Section 3, present the geometric calibration and rendering process Section 4, discuss the radiance calibration and image rendering in Section5, and demonstrate the experimental results of HDR image rendering and display in Section 6.
2. Multi-layer spatial modulation method for HDR-HMD displays
Figure 1 shows the schematic layout of two-dimensional (2D) HDR displays based on a multi-layer spatial light modulating scheme. A viewer sees a 2D image located at one of the SLM layers (e.g. the front layer SLM1), from which a cone of light is seen from each pixel of the image, and the other layers (e.g. the back layer SLM2) provide spatially-varying light modulation to enhance dynamic range. Without the loss of generalization, hereafter the front layer is referred to as the display layer and all the back layers adjacent to the light source are referred to as the modulation layer. It is worth noting the physical order of the display and modulation layers may be altered in an actual implementation. When the layers of SLMs are placed with negligible separations, the maximum command level that can be rendered by such a multi-layer architecture can be as high as (CL1*CL2*...CLN), where CL1, CL2, and CLN are the maximum command levels of the N-layers of the SLMs, respectively. It is worth pointing out that the number of distinctive command levels rendered by a multi-layer modulation architecture, depending on the rendering algorithms, is typically smaller than the maximum CL given by the product. Finally, a multi-layer modulation architecture gains the DR extension at a potential cost of compromised light efficiency considering the light loss through multiple SLMs. Therefore, increasing light efficiency is a critical factor of consideration when selecting the type and number of SLMs for the layers.
 Backlight
   Backlight
SLM 2
SLM 1
  SLM N SLM 1
     SLM 2
  SLM 1
   SLM 1 & 2
   Viewer
Viewer
(a)
(b)
Fig. 1. Schematic layout of multi-layer modulating scheme of 2D HDR display with two different cases. (a) two SLM layers perfectly overlay as they have same spatial resolution. The spatial frequency contents of the image are distributed equally in both layers and (b) two SLM layers are separated with a gap as they have different spatial resolution. The spatial frequency contents of the image are distributed unevenly in two layers.
The range and accuracy of the DR modulation highly depend on the spacing between the SLM layers. When the two SLMs of an HDR display provide the same spatial resolution, as illustrated in Fig. 1(a), making the two SLM layers perfectly overlay without any axial gap is highly desired, which offers pixel-by-pixel DR modulation and yields the maximum range and accuracy of DR enhancement. In this case, each pixel on the display layer is only
...
...

 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23323
modulated by a corresponding pixel on the modulation layer. When the spatial resolutions of the two SLMs do not match, as illustrated in Fig. 1(b), the higher-resolution SLM is typically viewed as the display layer for displaying contents of high spatial frequencies while the lower-resolution SLM is for providing low-frequency, spatially-varying light modulation, which is similar to the prototype by Seetzen et al utilizing a low-resolution LED array for spatially-varying modulation [3]. In this case, a small gap between the modulation layer and the display layer is desired such that the pixel structure of the low-resolution modulation layer is not perceivable and not degrades the overall image quality. With a non-negligible gap, a finite area on the modulation layer projects the cone of light seen from a pixel on the display layer and it highly depends on the separation of the two SLM layers and the numerical aperture of the HDR display to the viewer. When the projection area on the modulation layer is larger than the pixel size of the modulation layer, the perceived illuminance of each pixel on the display layer is simultaneously modulated by multiple pixels on modulation layer. Consequently, the maximal range and accuracy of dynamic range enhancement are compromised in comparison to pixel-by-pixel modulation approach. Furthermore, the low- resolution pixel structure of the modulation layer may have negative effects on the overall quality of the dual-layer modulated image. For instance, the pixel structure of the modulation layer may cast blurry but visible halo or shadow effects near the sharp edges of a displayed image.
Directly stacking two transmissive, miniature LCDs along with a backlight source may be considered as the most straightforward and compact adaption of the dual-layer modulation schemes for an HDR-HMD system, which would result in a hardware configuration similar to the light field stereoscope approach by Wetzstein et al [4]. Unlike the light field stereoscope approach where the SLM layers are spaced apart with a necessary gap for rendering the light field apparently emitted by an object in space by the directional light rays defined by pairs of pixels, the gap between the SLM layers for HDR rendering is subject to the considerations discussed above. In practice, however, the direct stacking method of multiple layers of SLMs suffers from several critical problems, which make it practically infeasible for HDR-HMD systems. Firstly, transmissive LCDs tend to have low dynamic range and low transmittance. The stacked dual-layer modulation would lead to very low light efficiency and limited dynamic range enhancement. Secondly, transmissive LCDs tend to have low fill factors and the microdisplays utilized in HMDs typically have pixels as small as a few microns, much smaller than the pixel size of about 100~500 microns for direct-view displays. As a result, the light transmitting through a two-layer LCD stack will inevitably suffer from severe diffraction effects and yield poor image resolution following an eyepiece magnification. Most importantly, due to the physical construction of typical LCD panels, the modulation layers of liquid crystal will be inevitably separated by a gap as large as a few millimeters, depending on the physical thickness of the cover glasses. For direct-view type desktop displays, a gap of a few millimeters between the two SLMs would not have much influence for dynamic range modulation. In an HMD system where the microdisplays are optically magnified by a large magnitude, even a gap as small as 1 millimeter in the SLM stack will be elongated by tens of times, resulting in a large separation in the viewing space, due to the HMD eyepiece magnification. The resulted separation in the visual space depends on the physical gap between the SLM layers and the focal length of the eyepiece. For example, assuming a focal length of 20mm for a typical HMD eyepiece and a typical viewing distance of 2 meters in the visual space, a 0.1mm gap in the SLM stack will lead to an axial separation as large as 0.6 meters. Unlike a tensor display where a gap between adjacent SLM layers is required for light field rendering, a large gap between adjacent SLM layers makes accurate dynamic range modulation practically impossible.

 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23324
 Fig. 2. Image reconstruction results of a dual-layer HDR-HMD and reconstructed image performance evaluation with the physical separation of the two SLMs set at 0.01mm and 2mm, respectively. (a) tone-mapped HDR target image, along with the intensity distribution along the dashed line; (b) the tone-mapped reconstructed HDR image; (c) binary noticeable difference map, where white areas denote the pixels with errors beyond the JND threshold; (d) PSF cross section at the display layer.
To illustrate how the quality of HDR enhancement is influenced by the physical gap between the SLMs, we simulated the reconstructed image performance as a function of the SLM separation. The grayscale map of a HDR target image is shown in Fig. 2(a), which is a sinusoid fringe pattern with decreasing spatial frequencies and damped image contrast in the horizontal and vertical directions, respectively. The target image has a spatial resolution of 1280 by 960 pixels with a 6.35um actual pixel pitch, which corresponds to the resolution of the hardware we used in Section 3. The maximum luminance and command level (CL) of the target HDR image are assumed to be 220 cd / m2 and 65535 (i.e. 16-bit depth), respectively. The front SLM is assumed to be the display layer, while the back SLM is assumed to be the modulation layer and its distance from front layer is varied to simulate the effects of the physical gap. A numerical aperture of 0.176 in the microdisplay space is assumed to view the HDR display. Figures 2(b).1 and 2(b).2 show the grayscale of the reconstructed HDR images by the two SLM layers with a gap of 0.01mm and 2 mm, respectively, between the two SLMs. The image intensity profiles along the dashed line in Fig. 2(a) are plotted underneath each image. Compared with the original intensity profile in Fig. 2(a), image contrast degradation is more noticeable in Fig. 2(b).2 with a larger gap than in Fig. 2(b).1 of a smaller gap. To simulate the HDR image reconstruction, the original HDR image was decomposed into two 8-bit images for the two SLMs based on the same rendering algorithm in Section 5. With a given gap between the two SLMs, the projected image of the back layer at the location of the display layer is blurred, which can be simulated by the aberration-free incoherent point spread function (PSF) of the defocused layer [7,8]:
∫12π 2π22 PSF(r,∆z)= 2 J0( NA⋅r⋅ρ)⋅expi σρ ρdρ 0λλ
with σ =2∆zsin2(α) 2
(1)
Where ∆z is the defocusing distance, which is the separation distance between the back layer and display layer, J0 is the zero order Bessel function, NA is the numerical aperture of

 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23325
the HDR image generator, α is the half angle of the emitting cone corresponding to NA , r is
the radial distance from the ray bundle center, λ is the wavelength, ρ is the normalized
integral variable on the exit pupil. The equivalent image of the defocusing modulation layer at the location of the display layer can be computed by convolving the PSF. The display layer content can be obtained by dividing the original target image with convolved image as a compensation.
We further computed the just noticeable difference (JND), which denotes the threshold intensity errors for all the command levels of a 16-bit image that are just distinguishable by the human visual perception based on the Barten’s contrast sensitivity function (CSF) model with the DICOM standard [9]. We then compared the reconstructed HDR image against the original target image and computed a binary noticeable difference map where a white pixel denotes that the intensity difference between the reconstructed and original HDR images is above the corresponding JND value and a dark pixel indicates the difference is below the threshold. Figures 2(c).1 and 2(c).2 show the binary noticeable difference maps corresponding to the 0.01mm and 2mm gap, respectively. The noticeable areas were 0 and 67.95% of the whole image for SLM gap of 0.01mm and 2mm, respectively. A significant increase in the noticeable errors is observed with the increase of SLM gap, especially in the region with high contrast and high spatial frequencies. Figures 2(d).1 and 2(d).2 further plot the PSFs of a back layer point on the display layer for the gap of 0.1 and 2mm, respectively, which characterize the intensity distribution of the light cone on the display layer. The light cone from a pixel on the modulation layer is projected to a circular region covering 1 and 12769 pixels for the gap of 0.01mm and 2mm, respectively, which shows a dramatically larger illuminated area and distribution change.
3. System design and prototype
Based on the analysis in Section 2, reducing the gap between the modulation and display layers becomes a critical consideration in the design of an HDR-HMD using a dual-layer modulation method. We proposed a new HDR-HMD system design in which ferroelectric liquid crystal on silicon (F-LCoS) microdisplays were used as the SLMs and an optical relay system was designed to optically minimize the physical separation between the SLMs. Compared to the drawbacks of low fill factor and low light efficiency of transmissive LCDs, reflective type F-LCoS microdisplays offer high pixel resolution with high fill factor as well as large contrast ratio and optical efficiency, which help to minimize diffraction artifacts and enhance dynamic range of the system [10]. The relay optics enables the system to optically overlay the SLMs with a minimal gap as small as a few microns and achieve pixel-by-pixel contrast modulation.
Figure 3 shows the schematic layout of a monocular HDR-HMD design, consisting of an
HDR image generator and viewing optics. The HDR image generator is composed of two F-
LCoS microdisplays by Miyota (FL1401) [11] and a custom relay system. The two F-LCoS
devices are 0.4” diagonally with a pixel resolution of 1280 by 960 and a 6.35um pixel pitch.
The LCoS1 has a built-in RGB LED source served as the system illumination and a built-in
wire grid film (WGF) served as the output polarizer, while we removed the built-in LED
source and WGF polarizer from the LCoS2. The LCoS1 unit offers a maximum luminance of 2
220 cd / m . A relay system with a 1:1 magnification and a polarizing beam splitter (PBS) were inserted between LCoS1 and LCoS2. The p-polarized light output by the LCoS1 transmits through the relay system and the PBS, is converged to form an intermediate image of LCoS1 coinciding with the LCoS2, and provides the per-pixel modulated illumination source for the LCoS2. With its built-in LED and the WGF cover removed, the LCoS2 modulates the incoming light and changes its polarization state based on the properties of LCoS microdisplays. The reflected s-polarized light modulated by the LCoS2 is then reflected by the PBS and propagates toward the viewing optics [12]. Given that both of the LCoS

 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23326
displays have an 8-bit depth for each color channel, the HDR image generator is anticipated to achieve a combined 16-bit modulation with an enhanced dynamic range beyond 60000:1.
Fig. 3. Schematic layout of a monocular HDR-HMD based on dual-layer modulation scheme.
One of the most critical aspects in designing an HDR-HMD system based on the scheme in Fig. 3 is the design of the relay system, because the final image quality highly depends on how well the optical resolution, contrast, and pixel geometry of the LCoS1 are retained by the relay system. Owing to the reflective nature of LCoS displays, it is highly desirable to achieve a double telecentric relay system that offers uniform illumination modulation, uniform optical magnification, and uniform light efficiency across the field of view (FOV). A double telecentric relay also offers some tolerance to small axial displacements between the images of the two SLMs. Based on the requirement of telecentricity and the incidence angle limit of ± 10° of the LCoS displays, an f/4 relay system was designed with all off-the-shelf lenses. The representative wavelengths were set to be 0.47, 0.55 and 0.61μm according to the dominant wavelengths of the RGB LED sources and were weighted as 1:3:1 based on the relative luminance response of the human visual system. Figure 4 shows the final optimization result of the HDR image generator, with the optical layout, polychromatic modulation transfer function (MTF), and f-tangent distortion shown in Figs. 4(a)-4(c) respectively. The MTF values are all above 0.25 at the cut-off frequency of 78.7 cycles/mm over the entire FOV. The system distortion is well corrected, less than −1.55% over the field. The residual distortion can be corrected by image processing which will be discussed in Section 4. The diameter of the relay tube is only 25mm and system total length is 123mm.
Figure 5 shows a bench prototype of the HDR-HMD system. The two LCoS panels were mounted onto two optical mount platforms with ±6° tip-tilt adjustments. The mechanical housing of the relay system was 3D printed and a commercial eyepiece was utilized as the viewing optics for magnifying HDR image. A grayscale camera with a focal length of 16mm was placed at the exit pupil of the eyepiece to replace an eye for image capturing.
 
 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23327
 Fig. 4. Optical design of an HDR image generator: (a) optical layout; (b) polychromatic modulation transfer function; and (c) distortion.
Fig. 5. Bench prototype of an HDR-HMD system based on the optical design in Fig. 4.
4. Geometrical calibration and rendering method
One of the most critical aspects in developing the prototype shown in Fig. 5 is to overlay the virtual images of the two SLMs with a substantially zero gap in between in order to achieve pixel-by-pixel high-accuracy modulation of dynamic range, high-resolution image, and high tolerance to the eye position within the exit pupil. Even small axial or lateral displacements can cause visible artifacts or resolution degradation. In practice, it is very challenging, if feasible at all, to achieve pixel-level alignment through pure mechanical adjustments of the LCoS panels, each of which has 6 degrees of freedom with translations and rotations. Moreover, the different optical paths of the two SLMs with respect to the shared eyepiece will introduce different optical distortions, which impose additional challenges for achieving perfect alignment through mechanical means.
To address these alignment challenges and achieve per-pixel modulation, we developed a camera-based calibration process. Figure 6 shows a simplified projection process where L1 and L2 represent the virtual images of the LCoS1 and LCoS2, respectively, viewed through the HMD optics, and the calibration camera located at O is assumed to be the global reference frame. A camera with a focal length of 12mm and an angular resolution of 0.44 arc
 
 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23328
minutes/pixel was placed at the exit pupil of the HMD and the lens distortion and intrinsic projection matrix of the camera was pre-calibrated using a well-established camera calibration method [13]. All captured images were digitally pre-warped using these calibrated parameters before applying display calibration. Through two separate calibration steps, we adopted our existing camera-based HMD calibration method [14] to obtain the inverse homographic mapping transformation matrices, Tcam ← L1 and Tcam ← L 2 , for planes L1 and L2,
respectively, which map a pixel on a given SLM on to a corresponding pixel of the calibration camera coordinate system [15]. Each of the two matrices consists of an extrinsic transformation matrix characterizing the positions and orientations of a given image plane (L1 or L2) with respect to the camera reference, and an intrinsic projection matrix characterizing the imaging properties of a corresponding HMD optical path. The calibration steps also obtain the optical distortion coefficients induced to the images of the SLMs. The first three orders of radial distortion and two orders of tangential distortion coefficients were calibrated by using camera-based HMD calibration mentioned in [13,14].
Fig. 6. Illustration of the projection process of points displayed in the virtual images of the SLMs onto the camera image plane (black vectors and coordinates are denoted in the camera global coordinate OXYZ).
Based on these calibrated parameters, a computational model is established for each of the SLM optical paths, through which a pixel displayed on one of the SLMs can be properly aligned with a corresponding pixel on the other SLM during the HDR image rendering process, to digitally correct alignment errors. Consider an image point P on the L2 and its corresponding location mapped on L1 is denoted as point Q. The computational model describing the mapping relationship of these two points can be simply expressed as:
(2)
 xL1 xL2 y y
 L1 −1  L2  0 =Tcam←L1⋅αt⋅Tcam←L2 0 
1 1  
where t= A2 +B2 +C2 Al + Bn + Cp
 Where (xL1,yL1) and (xL2,yL2) are the pixel coordinates of the points P and Q on the two
image planes L1 and L2, respectively, t is the parameter characterizing the ray vector mapping the pair of corresponding points P and Q, which is calculated by the normal vector (A,B,C) of the projection plane L1 and the reference point on L2 (l,n,p) in the camera global coordinate, α is a normalization factor which keeps the 4th dimension of the coordinate equals to 1. With the projection transformation relation in Eq. (2) and an appropriate

 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23329
interpolation method, the pixel-by-pixel mapping between the images on LCoS1 and LCoS2 can be established.
To digitally correct the misalignment between the two SLMs, the image to be displayed on of each SLM should be rendered individually by accounting for the corresponding homographic mapping matrix and distortion characteristics. Figure 7 illustrates the geometrical rendering procedures for two SLMs. As shown in Fig. 7(a), geometrically rendering the image for LCoS1 starts with pre-warping the pixel locations of an original image by applying the computational model expressed in Eq. (1), which enables pixel-by- pixel alignment of the LCoS1 against LCoS2 in the camera reference frame without accounting for optical distortions. The pre-warped image is further warped by applying the distortion correction. Geometrically rendering the LCoS2 image, shown in Fig. 7(a), is simpler than LCoS1 and it mainly requires an image flip operation for image parity change due to the odd number of reflections in the LCoS2 optical path and a pre-warp step for optical distortion correction.
Fig. 7. Geometric calibration procedure and alignment performance of a dual-layer HDR display. (a) geometric rendering procedure to create pre-warped images for LCoS1 and LCoS2 using a repetitive dot pattern as an input image; (b) error analysis after alignment procedure, Asterisk and circular dots stand for sampled coordinates on each LCoS. Arrow directions and magnitudes stand for the directions and relative magnitudes of the residual alignment errors; (c.1) and (c.2) photographs captured through the HDR system before applying the geometric pre-warping rendering and one after applying the correction, respectively.
To evaluate the alignment residual errors, we created a grid target sampling the entire field of view with 19*14 evenly-distributed positions. The original image was rendered using the procedures described above and shown in Fig. 7(a). The pre-warped images were displayed through the corresponding SLMs. To quantify the alignment error, we measured the magnitude and direction of the alignment errors of the 19*14 sampled points and Fig. 7(b) plotted the error map. The average of residual alignment errors can be less than 0.5 pixel after the whole alignment procedure. As a visual validation, two photographs captured by the calibration camera through the HMD system, one before applying the geometric pre-warping rendering and one after applying the correction, were shown in Figs. 7(c.1) and 7(c.2), respectively. The photograph for post-correction clearly demonstrated high-accuracy alignment of the calibration procedure.
 
 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23330
5. Radiance calibration and HDR image rendering
The second key aspect in developing the dual-layer HDR prototype shown in Fig. 5 is to calibrate the radiance response of the system so that radiance values stored in an HDR image can be correctly rendered as command levels and displayed through the two SLMs. The radiance response of the system, which establishes the relationship between the SLM command level and the resulted scene luminance, depends on not only the typically non- linear tone response curves of each SLM, but also the inherent inhomogeneity of the optical system, such as the system vegnetting, non-uniform illumination or pupil mismatch. Appropriate radiance correction should be implemented to obtain a desired level of radiance uniformity across the field of view.
Achieving the aforementioned radiance calibration across the field of view requires the calibration of a set of field-dependent tone response curves for each color channel in each SLM light path. Although in theory such calibration can be carried out by repetitively measuring the spectral luminance response of each color stimuli patch displayed at a given field position with a spectra-radiometer, such a field-by-field calibration for each of the color channels and each SLM is practically impossible not only due to the large repetition of measurements but also due to the practical challenges of aligning the display field position with the field of the spectra-radiometer. To overcome this challenge, we developed a three- step calibration process. The first step is to obtain the tone response curves of the central field for each SLM, the second step is to obtain a field-dependent uniformity correction map for the HDR display, and the third step is to obtain field-dependent tone response curves.
The calibration started with obtaining the tone response curves for the central field of each SLM path. A broad-band spectra-radiometer was placed at the center of the exit pupil of the HMD system to measure the spectral radiance of a small color patch displayed at the center of a SLM under calibration. By adopting a calibration procedure similar to the one in [3], the tone response curves of each color channel of each SLM path were obtained separately based on the spectral radiance measurements obtained for different color calibration patches. As an example, the normalized tone response curves of the central field of LCoS1 after piecewise cubic polynomial fitting are shown in Fig. 8(a).
The second-step calibration aims to render an apparently uniform image through the HDR display after applying correction to a uniform grayscale image. A radiance-calibrated camera was placed at exit pupil of the HMD optics to capture the modulated images of the SLMs on which images with equal grayscales (e.g. white background) were shown. A relative radiance response map of the HMD can be obtained from the captured image by applying the correction of the camera radiance response, from which a field normalization map f (i, j) ,
defined as the ratio of the luminance for each pixel (i, j) to the maximum luminance over the
entire field, was calculated, which will be utilized for field uniformity correction. Figure 8(b) plotted the field normalization map obtained through the calibration.
To achieve the field uniformity correction, the final step of calibration is to obtain the field-dependent tone response curves for a given field of each SLMs by truncating the tone response curves of the central field at the maximum command levels over the field normalizationmap, f(i,j),andthenscalingtherestpartofthecurves.Figure8(c)plottedan
example of the tone response curves of LCoS1 green channel corresponding to the central field and the four corner fields. Note that truncating the response curves actually reduce the contrast ratio and maximum command level to some extent. Thus, the tradeoffs between the field radiance uniformity and the image contrast ratio should be taken into account.

 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23331
 Fig. 8. The results of radiance response calibration and compensation. (a) red, green and blue channel response curves for LCoS1, (b) field normalization map obtained from radiance calibration, (c) tone response curves of LCoS1 green channel at the central field and four corner fields after radiance correction.
Following the above steps of radiance calibration, an HDR image storing the absolute luminance values of an HDR scene needs to be converted to two images with radiance- calibrated command levels for LCoS1 and LCoS2, respectively. The rendering algorithm began with equally splitting the input HDR image into two low-dynamic range modulation images by taking the square root of the input image. The luminance values of each modulation image were then converted to the command levels of corresponding pixels based on the corresponding field-dependent tone response curves obtained via the radiance calibration process.
6. Experimental results
To demonstrate the performance of the prototype, we captured a raw HDR image by a sequence of LDR raw images of an HDR scene with different exposures, as shown in Figs. 9(a)-9(k). The captured scene mainly consists of two USAF 1951 resolution targets and a desk lamp. The resolution target on the left side of the picture was printed on a white paper while the resolution target on the right side was printed on a transparency and mounted on a transparent plastic sheet. The lamp was placed right behind the targets, partially blocked by the left target while providing backlight to the transparent target on the right. The room lighting dimly illuminated the targets. Overall, the scene provides a wide range of luminance distribution as well as spatial details. It can be clearly seen from the pictures that the features of the light bulb and top-left corner of the transparent target were only captured with short exposures while the front targets in the shadow area were captured with long exposures. Radiance maps of the raw LDR images were then analyzed based on their radiance distribution and relative exposures. Finally, a single HDR image, which encodes the extended luminance values rather than the discrete command levels, was then synthesized from the multi-exposure radiance maps. To display the synthesized HDR image via LDR devices, the HDR image was tone-mapped into an 8-bit image shown in Fig. 9(l). Though the tone-

 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23332
mapped image shows a significantly lower image contrast than the HDR image, it indeed preserves all the spatial details.
Fig. 9. HDR image generation: (a)-(k) raw LDR images of an HDR scene captured with different camera exposure settings with the exposure time shown on the top-left corner of each image, (l) tone-mapped HDR image synthesized from the raw LDR images.
The synthesized HDR image was then re-rendered into two LDR modulation images, one for each SLM, by applying the radiance correction and rendering algorithm described in Section 5. The modulation images were then pre-warped by applying the geometric rendering algorithms in Section 4 for digital correction of misalignment and optical distortions. These geometrically corrected images were then displayed via their corresponding SLMs. To record the HDR image seen through the HDR-HMD, a camera of regular dynamic range was placed at the center of the exit pupil captured a series of images of four different exposures, shown in Fig. 10(a), to synthesize the dynamic range of the human visual system. Different parts of the scene were clearly captured by the different exposure settings, which demonstrated the fact that the HDR-HMD system was able to successfully render and display HDR contents. As a comparison, we created a LDR-HMD setup by disabling one of the SLM and only allowing a single layer modulation. The 8-bit tone-mapped image shown in Fig. 9(l) and one of the raw LDR images captured at a short exposure were respectively displayed through the LDR-HMD configuration. Following the same camera settings as in Fig. 10(a), we captured series of images and the results were shown in Figs. 10(b) and 10(c), respectively. Using the different sets of images shown in Figs. 10(a)-10(c) and following the same procedure as used for obtaining Fig. 9(l), we synthesized an image that merged the images from four exposure settings and applied the same tone-mapping techniques to the synthesized image. The results of the tone-mapped images from Figs. 10(a)-10(c) were shown in Fig. 10(d)-10(f), respectively. It is clear that the re-rendered HDR image displayed by the HDR-HMD was able to preserve the similar amount of details and dynamic range as the one shown in Fig. 9(l), while both the tone-mapped image and LDR image displayed by an LDR-HMD failed to preserve the details and dynamic range as expected.
 
 Vol. 25, No. 19 | 18 Sep 2017 | OPTICS EXPRESS 23333
 View publication stats
Fig. 10. Performance comparison of an HDR-HMD and a LDR-HMD. (a)-(c) images captured by a camera of different exposures through an HDR-HMD using an HDR image source, a LDR-HMD using a tone-mapped LDR image source, and a LDR-HMD using an LDR image under moderate exposure. (d)-(f) images synthesized and tone-mapped from the raw images in (a), (b) and (c), respectively.
7. Summary
In this paper, we presented the design and implementation of a high dynamic range head mounted display system using a dual-layer spatial modulation technique. By using two LCoS microdisplays as the spatial light modulators and optically overlaying their modulation planes by a relay system, we demonstrated the ability to create an HDR-HMD with high modulation accuracy. The paper further demonstrated the development of geometrical and radiance calibrations and rendering methods, which are two of the key enabling technical aspects for the proposed HDR-HMD architecture, and experimentally demonstrated the performance of the HDR system in comparison to an LDR-HMD. Future works can be done for further analyzing the image performances, system tolerances, increasing the luminance of backlighting, as well as improving the optics and electronics.
Funding
National Science Foundation (14-22653).
Disclaimer
Dr. Hong Hua has a disclosed financial interest in Magic Leap Inc. The terms of this arrangement have been properly disclosed to The University of Arizona and reviewed by the Institutional Review Committee in accordance with its conflict of interest policies.
  Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30539
Design and prototype of an augmented reality display with per-pixel mutual occlusion capability
AUSTIN WILSON AND HONG HUA*
3D Visualization and Imaging Systems Laboratory, College of Optical Sciences, University of Arizona, 1630 East University Boulevard, Tucson 85721, Arizona, USA
*hhua@optics.arizona.edu
Abstract: State-of-the-art optical see-through head-mounted displays for augmented reality (AR) applications lack mutual occlusion capability, which refers to the ability to render correct light blocking relationship when merging digital and physical objects, such that the virtual views appear to be ghost-like and lack realistic appearance. In this paper, using off- the-shelf optical components, we present the design and prototype of an AR display which is capable of rendering per-pixel mutual occlusion. Our prototype utilizes a miniature organic light emitting display coupled with a liquid crystal on silicon type spatial light modulator to achieve an occlusion capable AR display offering a 30° diagonal field of view and an angular resolution of 1.24 arcminutes, with an optical performance of > 0.4 contrast over the full field at the Nquist frequency of 24.2 cycles/degree. We experimentally demonstrate a monocular prototype achieving >100:1 dynamic range in well-lighted environments.
© 2017 Optical Society of America
OCIS codes: (120.2040) Displays; (120.2820) Heads-up displays; (330.7338) Visually coupled optical systems; (220.0220) Optical design and fabrication; (220.3620) Lens system design.
References and links
1. J. P. Rolland and H. Fuchs, “Optical Versus Video See-Through Head-Mounted Displays in Medical Visualization,” Presence (Camb. Mass.) 9(3), 287–309 (2000).
2. O. Cakmakci and J. Rolland, “Head-worn displays: a review,” J. Disp. Technol. 2(3), 199–216 (2006).
3. J. P. Rolland and H. Hua, “Head-mounted display systems,” in Encyclopedia of Optical Engineering, R. B.
Johnson and R. G. Driggers, eds. (Marcel Dekker 2005), pp.1–13.
4. H. Hua, C. Gao, L. D. Brown, N. Ahuja, and J. P. Rolland, “A testbed for precise registration, natural occlusion
and interaction in an augmented environment using a head-mounted projective display (HMPD),” Proceedings
IEEE Virtual Reality, Orlando, FL, pp. 81–89. (2002).
5. E. Tatham, “Getting the best of both real and virtual worlds,” Commun. ACM 42(9), 96–98 (1999).
6. A. Malmone and H. Fuchs, “Computational augmented reality eyeglasses,” Proc. of 2013 IEEE International
Symposium on Mixed and Augmented Reality (ISMAR), pp.29–38, (2013).
7. G. Wetzstein, D. Lanman, M. Hirsch, and R. Raskar, “Tensor Displays: Compressive light field synthesis using
multilayer displays with directional backlighting,” Proc. ACM SIGGRAPH (ACM Transaction on Graphics),
31(4), (2012).
8. K. Kiyokawa, M. Billinghurst, B. Campbell, and E. Woods, “An occlusion capable optical see-through head
mount display for supporting co-located collaboration,” Proceedings of The Second IEEE and ACM
International Symposium on Mixed and Augmented Reality, pp. 1–9, 2003.
9. K. Kiyokawa, Y. Kurata, and H. Ohno, “An Optical See-through Display for Mutual Occlusion with a Real-time
Stereo Vision System,” Elsevier Computer & Graphics Special Issue on “Mixed Realities - Beyond
Conventions,” 25(5), 2765–2779 (2001).
10. O. Cakmakci, Y. Ha, and J. P. Rolland, ” “A compact optical see-through head-worn display with occlusion
support,” Proceedings of the Third IEEE and ACM International Symposium on Mixed and Augmented Reality
(ISMAR), pp.16–25, (2004).
11. C. Gao, Y. Lin, and H. Hua, “Occlusion capable optical see-through head-mounted display using freeform
optics,” IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pp. 281–282, (2012)
12. C. Gao, Y. Lin, and H. Hua, “Optical see-through head-mounted display with occlusion capability,” Proc. SPIE
8735, 87350F (2013).
13. D. Cheng, Y. Wang, H. Hua, and M. M. Talha, “Design of an optical see-through head-mounted display with a
low f-number and large field of view using a freeform prism,” Appl. Opt. 48(14), 2655–2668 (2009).
14. D.Cheng,Y.Wang,H.Hua,andJ.Sasian,“Designofawide-angle,lightweighthead-mounteddisplayusing
free-form optics tiling,” Opt. Lett. 36(11), 2098–2100 (2011).
#305551 https://doi.org/10.1364/OE.25.030539
Journal © 2017 Received 1 Sep 2017; revised 16 Oct 2017; accepted 16 Oct 2017; published 21 Nov 2017
 
 Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30540
15. H. Hua, X. Hu, and C. Gao, “A high-resolution optical see-through head-mounted display with eyetracking capability,” Opt. Express 21(25), 30993–30998 (2013).
16. X. Hu and H. Hua, “High-resolution optical see-through multi-focal-plane head-mounted display using freeform optics,” Opt. Express 22(11), 13896–13903 (2014).
17. R. Zhang and H. Hua, “Design of a polarized head-mounted projection display using ferroelectric liquid-crystal- on-silicon microdisplays,” Appl. Opt. 47(15), 2888–2896 (2008).
1. Introduction
Augmented Reality (AR) is viewed as a transformative technology in the digital age, enabling new ways of accessing and perceiving digital information essential to our daily life. It is well embraced that the integration of AR technology with mobile computing will become as integrated as smart phones to all walks of life. A see-through head-mounted display (HMD) is one of the key enabling technologies for merging digital information with a physical scene in an AR system [1]. While both video see-through and optical see-through displays have their unique advantages, optical see-through HMDs (OST-HMD) tend to be preferred when it comes to real scene resolution, viewpoint disparity, FOV and image latency [1].
Developing OST-HMDs, however, presents many technical challenges [2,3], one of which lies in the challenge of correctly rendering mutual occlusion relationships between digital and physical objects in space. Mutual occlusion is the light blocking behavior when intermixing virtual and real objects—an opaque virtual object should appear to be fully opaque and occlude a real object located behind it and a real object should naturally occlude the view of a virtual object located behind the real one. There are two types of occlusion: that of real-scene objects occluding virtual ones, and of virtual objects occluding the real scene. While the occlusion of a virtual object by a real object can be achieved straightforwardly, by simply not rendering the virtual object where the occluding real object sits, when the location of the real object relative to the virtual scene is known, the occlusion of a real object by a virtual one presents a much more complicated problem because it requires the blocking of light in the real scene. The state-of-the-art OST-HMDs typically rely upon a beam splitter (BS) to uniformly blend the light from the real scene with the virtual objects, and lack the ability to selectively block out the light of the real world from reaching the eye. As a result, the digitally rendered virtual objects viewed through OST-HMDs typically appear “ghost-like,” always floating “in front of” the real world. Figure 1 shows an un-edited AR view captured by a camera through a typical OST-HMD lacking occlusion capability where the virtual airplane appears not only washed out and non-opaque but also low-contrast.
Fig. 1. Superimposing a virtual airplane in a well-lit real world environment: AR view captured through a typical OST-HMD without occlusion capability.
Creating a mutual occlusion-capable optical see-through HMD (OCOST-HMD) poses a complex challenge. In the last decade, few OCOST-HMD concepts have been proposed, with even fewer designs being prototyped [4–8]. The existing methods for implementing OCOST-
  
 Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30541
HMDs fall into two types: direct ray blocking and per-pixel modulation. The direct ray blocking method selectively blocks the rays from the see-through scene without focusing them. It can be implemented by selectively modifying the reflective properties of physical objects or by passing the light from the real scene through a single or multiple layers of spatial light modulators (SLM) placed directly near the eye. For instance, Hua et al. investigated the idea of creating natural occlusion of virtual objects by physical ones via a head-mounted projection display (HMPD) device, which involved the use of retroreflective screens onto non-occlusion physical objects and thus can only be used in limited setups [4]. Tatham demonstrated the occlusion function through a transmissive SLM directly placed near the eye with no imaging optics [5]. The direct ray blocking method via an SLM would be a straightforward and adequate solution if the eye were a pinhole aperture allowing a single ray from each real-world point to reach the retina. Instead, the eye has an area aperture, which makes it practically impossible to block all the rays seen by the eye from an object without blocking the rays from other surrounding objects using a single-layer SLM. Recently, Maimone and Fuchs proposed a lensless computational multi-layer OST-HMD design which consists of a pair of stacked transmissive SLMs, a thin and transparent backlight, and a high- speed optical shutter [6]. Multiple occlusion patterns can be generated using a multi-layer computational light field method [7] so that the occlusion light field of the see-through view can be rendered properly. Although the multi-layer light field rendering method can in theory overcome some of the limitations of a single-layer ray blocking method, it is subject to several major limitations such as the significantly degraded see-through view, limited accuracy of the occlusion mask, and the low light efficiency. The unfavorable results can be attributed to the lack of imaging optics, low light efficiency of the SLMs, and most importantly the severe diffraction artifacts caused by the fine pixels of the SLMs located at a close distance to the eye pupil.
The per-pixel occlusion method, as illustrated in Fig. 2, is to form a focused image of the see-through view at a modulation plane where an SLM is inserted and renders occlusion masks to selectively block the real-world scene point by point. Based on this principle, the ELMO series of prototypes designed by Kiyokawa et. al. in the early 2000’s perhaps are still the most complete demonstration of OST-HMDs with occlusion capabilities [8,9], all of which were implemented using conventional lenses, prisms and mirrors. The ELMO-4 prototype contains 4 lenses, 2 prisms and 3 optical mirrors arranged in a ring structure that presents a very bulky package blocking most of the user’s face. Limited by the microdisplay and SLM technologies at that time, the ELMO prototypes have fairly low resolutions for both the see-through and virtual display paths, both of which used a 1.5-inch QVGA (320x240) transmissive LCD module [8,9]. Using a transmissive LCD as a SLM becomes problematic because when coupled with a polarizing beamspliter (PBS), it allows for minimal light (<20%) from the real scene to pass through to the user, causing the device to become ineffective in dim environments. Cakmakci et al attempted to improve the compactness of the overall system by utilizing polarization-based optics and a reflective SLM [10]. They used a reflective liquid crystal on silicon (LCoS) in conjunction with an organic light emitting device (OLED) display to give an extended contrast ratio of 1:200. An x-cube prism was proposed for the coupling of the two optical paths to achieve a more compact form factor. However, the design failed to erect the see-through view correctly [10]. Recently, Gao et al. proposed to use freeform optics, a two-layer folded optical architecture, along with a reflective SLM to create a compact high resolution, low distortion OCOST-HMD [11,12]. With the utilization of a reflective LCoS device as the SLM, the system allowed for a high luminance throughput and high optical resolution for both virtual and see-through paths. The optical design and preliminary experiments demonstrated great potentials for a very compelling form factor and high optical performances, but the design was dependent on the use of expensive freeform lenses and, regrettably, was not prototyped. Although freeform lenses can make it possible to

 Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30542
create compact, wide field-of-view (FOV) eyepiece designs needed for occlusion-capability, these lenses are often expensive and challenging to design and fabricate [13–17].
In this paper, based on the two-layer folding optics architecture by Gao et al. [11,12], we present the design and prototype of a high-resolution, affordable OCOST-HMD system using off-the-shelf optical components. Our prototype, capable of rendering per-pixel mutual occlusion, utilizes an OLED microdisplay for the virtual display path coupled with a reflective LCoS as the SLM for the see-through path to achieve an occlusion capable OST- HMD offering a 30 degree diagonal FOV and 1920x1080 pixel resolution, with an optical performance of greater than 20% modulation contrast over the full FOV. We experimentally demonstrate a monocular prototype achieving >100:1 dynamic range in well-lighted environments. We further experimentally compared the optical performance of an OST-HMD with and without occlusion capability.
2. System optical design
Figure 2 illustrates a schematic diagram of our proposed OSOST-HMD optical architect. The design uses two folding mirrors, a roof prism and a PBS to fold the optical paths into a two- layer design, where the occlusion and the virtual display modules share the same eyepiece, giving a compact form factor and enabling per-pixel occlusion capability. The light path for the virtual display is highlighted with blue arrows, while the light path for the real-world view is shown with red arrows. An objective lens collects the light from the physical environment and forms an intermediate image at its focal plane where an amplitude-based SLM is placed to render an occlusion mask for controlling the opaqueness of the real view. The modulated light is then folded by a PBS toward an eyepiece for viewing. The PBS acts as a combiner to merge the light paths of the modulated real view and virtual view together so that the same eyepiece module is shared for viewing the virtual display and the modulated real-world view. The focal planes of the eyepiece and objective are optically conjugate with each other, which makes it possible to individually control the opaqueness of each individual pixel of the virtual and real scenes for pixel-by-pixel occlusion manipulation. A right-angle roof prism is utilized to not only fold the optical path of the real view for compactness but also to ensure an erected see-through view which is another critical requirement for an OCOST-HMD system. The system may further integrate a depth sensor that obtains the depth map of a real-world scene in order to generate a scene-dependent occlusion mask in real time.
Fig. 2. Schematic diagram of the proposed OCOST-HMD design based on two-layer folded architecture.
After comparing several candidate microdisplay technologies, we chose a 0.7” Sony color OLED microdisplay for the virtual display path. The Sony OLED, having an effective area of
 
 Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30543
15.5mm and 8.72mm and a pixel size of 12μm, offers a native resolution of 1280x720 pixels and an aspect ratio of 16:9. Ideally we would need an SLM of the same dimension, aspect ratio and pixel resolution to achieve pixel-by-pixel occlusion capability within the entire FOV of the virtual display. Limited by the availability of an SLM of the same specifications, we selected a 0.7” LCoS as the SLM for the see-through path. The LCoS, recycled from a Canon projector, offers a native resolution of 1400x1050 pixels, a pixel pitch of 10.7μm, and an aspect ratio of 4:3. A reflective SLM provides a substantial advantage in light efficiency and contrast over a light transmitting SLM. Typically, the light efficiency of the see-through path can be as high as 45% with a reflective LCoS but about 10% or less with a transmissive SLM, while the blocking efficiency is about 0.009% for a reflective SLM and 0.02% for a transmissive SLM [11]. Consequently, by using a reflective type SLM, twice the blocking efficiency can be achieved. In addition, diffraction artifacts resulted from the propagation of light through an aperture is negligible for an SLM with a high fill factor while it is substantially noticeable for a transmissive LCD which typically has a low fill factor.
Based on the choices of microdisplay and SLM, we aimed to achieve an OCOST-HMD prototype with a diagonal FOV of 30°, or 26.5° horizontally and 15° vertically, and an angular resolution of 1.24 arcmins per pixel, corresponding to a Nyquist frequency of 24.2 cycles/degree in the visual space. We also set the goal of achieving an exit pupil diameter (EPD) of 9-12mm, allowing eye rotation of about ± 25° within the eye socket without causing vignetting of the optical system, and an eye clearance distance of at least 18mm. In order to develop a high-performance prototype with substantially much less cost than that of freeform optics in [11,12], we chose to carry out the entire optical design using available stock lenses, which makes the task substantially more challenging due to very limited choices of lens shapes and glass types. These constraints need to be carefully considered during the optimization process when creating lens forms for the eyepiece and objective designs. Furthermore, an optimized design obtained via an optical design software needs to be carefully matched and replaced by catalog lenses, which typically is subject to an iterative process of optimization and replacement. The design was further complicated due to the choice of a reflective SLM which requires an image-space telecentricity for both the eyepiece and objective designs to achieve high contrast, light efficiency and image uniformity. The final challenge of the design is the requirement for a large back focal distance (BFD) to make enough space for combining the two optical paths via a PBS.
Fig. 3. Optical system layout of OCOST-HMD using stock lenses.
Figure 3 shows the lens layout of the final OCOST-HMD design. The light path for the virtual display (eyepiece) is denoted by the blue rays, while the light path for the see-through view is shown in red rays. It should be noted that the red rays for the see-through view
 
 Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30544
overlap with the blue rays of the eyepiece after the PBS and thus only the blue rays are traced to the eye pupil in Fig. 3. The final design consists of 11 glass lenses (2 flint and 9 crown glass), 2 folding mirrors, 1 PBS, and 1 roof prism, all of which are stock components except for the meniscus which is made of flint glass with an aperture diameter greater than 40mm. Chromatic aberrations were optimized for 465, 550, and 615nm with weights of 1, 2, and 1, respectively, according to the dominant wavelengths of the microdisplay. The objective was optimized to have the chief ray deviated less than ± 0.5° from a perfect telecentric system while ± 1° deviation was allowed for the eyepiece. After properly cropping the eyepiece lenses, we were able to achieve an eye clearance of 18mm and a 10mm EPD.
The optical performance of the virtual display and see-through paths were assessed over the full field of view in the visual space where the spatial frequencies are characterized by the angular size in terms of cycles per degree. Figure 4 shows the polychromatic modulation transfer function (MTF) curves, evaluated with a 3-mm eye pupil, for several weighted fields of both the virtual display and the see-through paths. The virtual display path preserves roughly 40% modulation at the designed Nyquist frequency of 24.2 cycles/degree, corresponding to the 12μm pixel size of the OLED display. It can even maintain about 20% modulation at the frequency of 36 cycles/degree for the potential to update to an OLED of 8μm pixel size and 1920x1080 pixels. The performance of the see-through path has dropped slightly to an average modulation of 35% for the frequency of 25 cycles/degree and maintains about 30% modulation at the frequency of 30 cycles/degree for >90% of the entire see- through field except that the MTF of the very far edge field drops to about 15%. Such optical performance is comparable to or even better than many custom HMD optics of similar resolution.
Fig. 4. Modulation transfer function of 0, 5, 10 and 15 degree transverse (Tan) and radial (Rad) fields evaluated with a 3mm pupil diameter and a cutoff spatial frequency of 36 cycles/degree for the (a) OCOST-HMD eyepiece design and 60 cycles/degree for the (b) OCOST-HMD see- through (i.e. eyepiece + objective) design.
Along with the MTF, the wavefront error plot and spot diagram for the see-through and virtual display paths were used to characterize the performance of the optical design. For the virtual display path, the dominating aberrations are coma and lateral chromatic aberration. While lateral chromatic aberration can be digitally corrected, much like distortion correction, by pre-warping the image for the red and blue color channels individually based on their laterial displacements from the reference green color channel, coma is exceptionally hard to correct. This is due to the non-pupil forming, telecentric design of the eyepiece and the inability to move the stop position to balance off-axis aberrations. Overall, the wavefront aberration in the eyepiece is sufficiently low, being under 1 wave. The average root mean square (RMS) spot diameter across the field is 15μm. Although it appears to be larger than the 12μm pixel size, this difference is largely due to lateral chromatic aberration, which as stated
 
 Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30545
earlier, can be corrected. The dominating aberration in the objective lens design is axial chromatic aberration, which is typically corrected by using different glass types to balance the optical dispersion. Unfortunately, due to the limited flint glass selection of off-the-shelf lenses, this aberration is unavoidable. Nevertheless, the maximum wavefront aberration in the real image is still below 2 waves at the far field, and the average RMS spot diameter across the field is about 19μm. Compared to the 10.7 μm pixel pitch of the LCoS being used in the system, a 19μm RMS spot diameter in the objective design indicates that the actual occlusion mask resolution is limited by the objective lens resolution and is lower than the pixel resolution of the SLM.
3. System prototype and experimental demonstration
Figure 5(a) shows the sectional view of the mechanical housing with the light path of the real scene superimposed. For the mechanical design, lens cell stacks were used and inserted into a larger housing, where they were held by set screws to achieve more compensation in the optical design and meet the maximum MTF. Figure 5(b) shows the monocular prototype of the OCOST-HMD system built upon the optical design in Fig. 3. The prototyped system was measured as 82mm in height, 70mm in width, 50mm in depth.
Fig. 5. (a) Solidworks design of the fully assembled OCOST-HMD (b) Monocular prototype of the OCOST-HMD.
The vertical and horizontal FOV was determined for both the virtual and real paths by viewing a ruler through the optical system. It was determined that the see-though FOV was 27.69° horizontally and 18.64° vertically with an occlusion capable see-through FOV 22.62° horizontally and 17.04° vertically, while the virtual display had an FOV of 26.75° horizontally and 15.19° vertically, giving a measured diagonal Full FOV of 30.58 °. Due to the slightly mismatched aspect ratio between the OLED and LCoS, we anticipated that the LCoS would not be able to occlude the real scene in the same FOV of the virtual display in the horizontal direction.
For the purpose of qualitative demonstration of the occlusion capability of the OCOST- HMD prototype, we created a real-world scene composed of a mixture of laboratory objects with a well-illuminated white background wall (~300-500 cd/m2) while the virtual 3D scene was a simple image of a teapot. Figures 6(a) through 6(f) show a set of images captured with a digital camera placed at the exit pupil of the eyepiece. The camera lens has a focal length of 16mm with its aperture set at about 3mm to match the F/# setting equivalent to that of human eyes under typical lighting conditions. Figure 6(a) is the view of the natural background scene only captured through the occlusion module when the SLM is turned on for light pass-through without a modulation mask applied and with the OLED microdisplay turned off. Several different spatial frequencies and object depths were portrayed in the background scene to
 
 Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30546
display image quality and depth cues. Figure 6(b) is the view of the virtual scene captured through the eyepiece module when the real-word view was completely blocked by the SLM.
 Fig. 6. Experimental demonstration of mutual occlusion capability in our OCOST-HMD prototype with photographs captured with a digital camera placed at the exit pupil of the system: (a) view of a natural background scene through the occlusion model for light pass- through with the SLM turned on; (b) view of the virtual scene through the eyepiece with the see-through path being blocked by the SLM; (c) augmented view of the natural and virtual scenes without occlusion capability enabled; (d) View of the natural scene with an occlusion mask rendered on SLM; (e) augmented view with occlusion capability enabled where the virtual teapot is inserted in front of the background scene; (f) augmented view with occlusion capability enabled where the virtual teapot is inserted between two real objects for mutual occlusion demonstration.
Figure 6(c) shows the augmented view of the real-world and virtual scenes without the occlusion capability enabled (i.e., no modulation mask was applied to the SLM) by simply turning on the OLED microdisplay. Due to the bright environment, the teapot looks washed out without a mask occluding the see-though path. Not only does the teapot appear unrealistic and ghost-like, but it is also spatially unclear where the teapot sits in the image. Clearly, the virtual and real objects are mixed in very low contrast, which is the expected effect obtained through a typical OST-HMD without occlusion capability. Figure 6(d) shows the view of the real-world scene when the occlusion mask was displayed on the SLM and no virtual content shown on the OLED display. Apparently, the mask could effectively block the portion of the

 Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30547
see-through view. Figure 6(e) is a view captured with the mask on the SLM and the virtual scene displayed on the OLED display. The result clearly demonstrates improved contrast and quality for the virtual view. We can observe that a realistic virtual image with obvious depth cues is now present. When virtual objects occlude the real scene, viewers can seamlessly transfer from AR to VR environments. To demonstrate the full capability and correct depth perception the occlusion display can render, Fig. 6(f) shows the view captured with the see- through path, where the virtual teapot is inserted between two real objects, demonstrating the mutual occlusion capability of the system. In this case, knowing the relative location of the can which is meant to occlude part of the teapot, we removed the pixels that correspond to the projection of the occluding can on the virtual display from the teapot rendering. The significance of the result is that correct occlusion relationships can be created and used to give an unparalleled sense of depth to a virtual image in an OST-HMD. With a dynamic range of the virtual scene in bright environments, our OCOST-HMD system using stock lenses achieved a high optical performance, one that has significantly increased over that of non-occlusion-capable HMD designs.
4. Optical performance test
To further quantify the optical performance of the prototype system, we started with characterizing the MTF performance of virtual and real light paths through the prototype. A high-performance camera, consisting of a nearly diffraction-limited 16mm camera lens by Edmund Optic and a 1/3” Point Grey image sensor of a 3.75 μm pixel pitch was placed at the exit pupil of the system. It offers an angular resolution of about 0.8 arcminutes per pixel, significantly higher than the anticipated performance of the prototype. Therefore, it is assumed that no loss of performance to the MTF was caused by the camera. The camera then captures images of a slanted edge target, which is either displayed by the microdisplay or a printed target placed in the see-through view. To provide a separable quantification of the performance for the virtual and see-through path, the virtual image of a slanted edge was taken while the see-through scene was completely blocked by the SLM. Similarly, the see- through image of the target was taken with the microdisplay turned off. The captured slanted- edge images were analyzed using Imatest software to obtain the MTF of the corresponding light paths.
Fig. 7. Measured MTF performance of the OCOST-HMD prototype for the on-axis field of the virtual display, see-through view as well as the camera used for measurement.
 
 Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30548
Figure 7 shows that the measured on-axis MTF performance of both virtual and real paths, along with the MTF of the camera itself without the system for comparison, which match closely with the nominal performance shown in Fig. 4. Due to the magnification difference between the pixel pitch of the camera sensor and the microdisplay and SLM, the horizontal axis of the MTF measurement by Imatest was scaled by the pixel magnification difference between the camera and display and then converted to define the spatial frequency in the visual space in terms of cycles/degree by computing the angular size of a spatial feature, making it directly comparable with the plots in Fig. 4. The prototyped design was able to achieve a contrast greater than 40% at the Nyquist frequency 24.2 cycles/degree of the virtual display and similar performance for the see-through path. We then directly measured the spatial and angular resolutions of the see-though path using a printed US1951 resolution target. The target was set at 60cm away from the exit pupil and the same camera was used to capture a see-through image of the target to directly determine the smallest resolvable group. A contrast ratio above 0.1 was determined to be resolvable. The resolvable spatial frequency was determined to be at the Group 2 Element 5 for both horizontal and vertical lines, corresponding to 6.35 cycles/mm. At a distance of 60cm, this element gives an angular resolution of 66.49 cycles/degree, indicating that the resolvability of see-through path through the occlusion module is nearly intact to a human viewer.
We further measured the image contrast between the virtual display and the real-world
scene as a function of the real-world scene brightness for different spatial frequencies. A
grayscale solid image, ranging from black to white in 10 linear steps, was displayed on an
LCD monitor to create a controlled background scene with varying luminance from 0 to
2
350cd/m . The monitor was placed roughly 10cm in front of the OCOST-HMD system to
simulate an array of real scene brightness. A sinusoidal grating pattern with a spatial frequency ranging from 0.7 to 24.2 cycles/degree was displayed on the OLED microdisplay (virtual path) to evaluate the effect of scene brightness on the image contrast of the virtual scene at different spatial frequencies. The fall-off in contrast to the virtual scene was then plotted and compared with occlusion enabled (SLM blocking see-through light) and without occlusion (SLM passing see-through light). Figures 8(a) and 8(b) show the captured images of a 12 cycles/degree spatially varying virtual image superimposed on a background image of full brightness with and without occlusion, respectively. Without occlusion, the virtual target
2
was nearly washed out completely with a background as bright as 350 cd/m . Figures 9(a) and
9(b) plotted the contrast of the virtual object contrast with the see-through path un-occluded
and occluded, respectively. We can observe that the contrast of the virtual object without
occlusion is quickly deteriorated to zero for a well-lit environment luminance above 200
2
cd/m , while the contrast of the virtual target with occlusion of the real scene is nearly
constant over an increased brightness. We further measured the obtainable contrast ratio of the occlusion system is greater than 100:1. The contrast ratio of the occlusion capable display was obtained by measuring a collimated depolarized light source through the system with full occlusion being enabled and disabled.

 Vol. 25, No. 24 | 27 Nov 2017 | OPTICS EXPRESS 30549
 Fig. 8. Sample images of a grating target of 12 cycles/degree displayed by the virtual display superimposed onto a bright background of 350cd/m2 (a) with occlusion enabled to block the see-through light and (b) without occlusion.
 Fig. 9. Image contrast degradation of the virtual target of different spatial frequencies as a function of background scene brightness for (a) occlusion-disabled; and (b) occlusion-enabled displays.
5. Conclusion
This paper presents a novel design and implementation of an occlusion-capable optical see- through head-mounted display system using off-the-shelf optical components. A comprehensive description of the design and the monocular prototype was included, and the performance of the prototype was analyzed and evaluated. The system offered a 30° diagonal FOV and an angular resolution of 1.24 arcmins, with an optical performance of > 0.4 contrast over the full FOV at the Nyquist frequency of the display. By using the combination of a reflective type SLM and OLED display, we demonstrated a contrast ratio greater than 100:1 for the occlusion module. We also demonstrated that our prototype could be used in bright environments without loss of contrast to the virtual image. This study demonstrates that an OCOST-HMD system can achieve a high optical performance and a compact form factor for bright environments while using off-the-shelf components.
Disclaimer
Dr. Hong Hua has a disclosed financial interest in Magic Leap Inc. The terms of this arrangement have been properly disclosed to The University of Arizona and reviewed by the Institutional Review Committee in accordance with its conflict of interest policies.
 